{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Performs Research with Multi-Agent Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.2.0b4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k-0314',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\",\n",
    ")\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code.\"\"\",\n",
    ")\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "The plan may involve an engineer who can write code and a scientist who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.\n",
    "\"\"\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"paper\"},\n",
    ")\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    system_message=\"Critic. Double check plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source URL.\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor, critic], messages=[], max_round=50\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape arXiv for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data, read the abstracts, and categorize the papers into different domains based on the content.\n",
      "\n",
      "3. Engineer: Write a script to convert the categorized data into a markdown table. The table should include columns for the title, authors, domain, and link to the paper.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the information is accurate and the categorization is correct.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the markdown table based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Submit the final markdown table to the admin for approval.\n",
      "\n",
      "7. Admin: Review the markdown table and provide feedback or approve the table.\n",
      "\n",
      "8. Engineer and Scientist: Make any necessary revisions based on the admin's feedback and resubmit for approval. Repeat this step until the admin approves the table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan is well-structured and covers all the necessary steps to achieve the goal. However, it lacks the inclusion of verifiable information such as source URLs. Here's a revised version of the plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape arXiv for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data, read the abstracts, and categorize the papers into different domains based on the content. Make sure to note down the source URL for each paper.\n",
      "\n",
      "3. Engineer: Write a script to convert the categorized data into a markdown table. The table should include columns for the title, authors, domain, link to the paper, and source URL.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the information is accurate and the categorization is correct. Verify that the source URLs are correctly linked.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the markdown table based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Submit the final markdown table to the admin for approval, ensuring that the source URLs are included.\n",
      "\n",
      "7. Admin: Review the markdown table and provide feedback or approve the table.\n",
      "\n",
      "8. Engineer and Scientist: Make any necessary revisions based on the admin's feedback and resubmit for approval. Repeat this step until the admin approves the table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan is good. Let's start with step 1: Write a script to scrape arXiv for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "To accomplish this task, we can use the arXiv API. The API allows us to search for papers based on various criteria, including the title, authors, and abstract. We can also filter the results based on the date of publication to only include papers published in the last week.\n",
      "\n",
      "Here is a Python script that uses the arXiv API to search for papers related to LLM applications published in the last week:\n",
      "\n",
      "```python\n",
      "import feedparser\n",
      "import datetime\n",
      "\n",
      "# Define the search parameters\n",
      "search_query = 'all:LLM'\n",
      "start_index = 0\n",
      "max_results = 100\n",
      "\n",
      "# Get the current date and time\n",
      "now = datetime.datetime.now()\n",
      "\n",
      "# Calculate the date one week ago\n",
      "one_week_ago = now - datetime.timedelta(weeks=1)\n",
      "\n",
      "# Convert the date to the format required by the arXiv API\n",
      "date = one_week_ago.strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Define the URL for the arXiv API\n",
      "url = f'http://export.arxiv.org/api/query?search_query={search_query}&start={start_index}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "\n",
      "# Use the feedparser library to parse the RSS feed\n",
      "feed = feedparser.parse(url)\n",
      "\n",
      "# Create a list to store the papers\n",
      "papers = []\n",
      "\n",
      "# Loop through the entries in the feed\n",
      "for entry in feed.entries:\n",
      "    # Get the date of publication\n",
      "    published = datetime.datetime.strptime(entry.published, '%Y-%m-%dT%H:%M:%SZ')\n",
      "\n",
      "    # Check if the paper was published in the last week\n",
      "    if published > one_week_ago:\n",
      "        # Extract the title, authors, abstract, and link\n",
      "        title = entry.title\n",
      "        authors = ', '.join(author.name for author in entry.authors)\n",
      "        abstract = entry.summary\n",
      "        link = entry.link\n",
      "\n",
      "        # Add the paper to the list\n",
      "        papers.append({\n",
      "            'title': title,\n",
      "            'authors': authors,\n",
      "            'abstract': abstract,\n",
      "            'link': link\n",
      "        })\n",
      "\n",
      "# Print the papers\n",
      "for paper in papers:\n",
      "    print(paper)\n",
      "```\n",
      "\n",
      "This script will print a list of dictionaries, where each dictionary represents a paper and contains the title, authors, abstract, and link.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 1, in <module>\n",
      "    import feedparser\n",
      "ModuleNotFoundError: No module named 'feedparser'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems like the Python environment doesn't have the `feedparser` module installed, which is required for parsing the RSS feed from the arXiv API. \n",
      "\n",
      "Let's modify the script to use the `requests` and `bs4` (BeautifulSoup) modules instead, which are more commonly installed in Python environments. \n",
      "\n",
      "Here is the modified script:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import datetime\n",
      "\n",
      "# Define the search parameters\n",
      "search_query = 'all:LLM'\n",
      "start_index = 0\n",
      "max_results = 100\n",
      "\n",
      "# Get the current date and time\n",
      "now = datetime.datetime.now()\n",
      "\n",
      "# Calculate the date one week ago\n",
      "one_week_ago = now - datetime.timedelta(weeks=1)\n",
      "\n",
      "# Convert the date to the format required by the arXiv API\n",
      "date = one_week_ago.strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Define the URL for the arXiv API\n",
      "url = f'http://export.arxiv.org/api/query?search_query={search_query}&start={start_index}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "\n",
      "# Send a GET request to the arXiv API\n",
      "response = requests.get(url)\n",
      "\n",
      "# Parse the response using BeautifulSoup\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "# Create a list to store the papers\n",
      "papers = []\n",
      "\n",
      "# Loop through the entries in the feed\n",
      "for entry in soup.find_all('entry'):\n",
      "    # Get the date of publication\n",
      "    published = datetime.datetime.strptime(entry.published.string, '%Y-%m-%dT%H:%M:%SZ')\n",
      "\n",
      "    # Check if the paper was published in the last week\n",
      "    if published > one_week_ago:\n",
      "        # Extract the title, authors, abstract, and link\n",
      "        title = entry.title.string\n",
      "        authors = ', '.join(author.string for author in entry.find_all('name'))\n",
      "        abstract = entry.summary.string\n",
      "        link = entry.id.string\n",
      "\n",
      "        # Add the paper to the list\n",
      "        papers.append({\n",
      "            'title': title,\n",
      "            'authors': authors,\n",
      "            'abstract': abstract,\n",
      "            'link': link\n",
      "        })\n",
      "\n",
      "# Print the papers\n",
      "for paper in papers:\n",
      "    print(paper)\n",
      "```\n",
      "\n",
      "This script does the same thing as the previous script, but it uses `requests` and `bs4` instead of `feedparser`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "{'title': 'PoseGPT: Chatting about 3D Human Pose', 'authors': 'Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black', 'abstract': \"  We introduce PoseGPT, a framework employing Large Language Models (LLMs) to\\nunderstand and reason about 3D human poses from images or textual descriptions.\\nOur work is motivated by the human ability to intuitively understand postures\\nfrom a single image or a brief description, a process that intertwines image\\ninterpretation, world knowledge, and an understanding of body language.\\nTraditional human pose estimation methods, whether image-based or text-based,\\noften lack holistic scene comprehension and nuanced reasoning, leading to a\\ndisconnect between visual data and its real-world implications. PoseGPT\\naddresses these limitations by embedding SMPL poses as a distinct signal token\\nwithin a multi-modal LLM, enabling direct generation of 3D body poses from both\\ntextual and visual inputs. This approach not only simplifies pose prediction\\nbut also empowers LLMs to apply their world knowledge in reasoning about human\\nposes, fostering two advanced tasks: speculative pose generation and reasoning\\nabout pose estimation. These tasks involve reasoning about humans to generate\\n3D poses from subtle text queries, possibly accompanied by images. We establish\\nbenchmarks for these tasks, moving beyond traditional 3D pose generation and\\nestimation methods. Our results show that PoseGPT outperforms existing\\nmultimodal LLMs and task-sepcific methods on these newly proposed tasks.\\nFurthermore, PoseGPT's ability to understand and generate 3D human poses based\\non complex reasoning opens new directions in human pose analysis.\\n\", 'link': 'http://arxiv.org/abs/2311.18836v1'}\n",
      "{'title': 'InstructSeq: Unifying Vision Tasks with Instruction-conditioned\\n  Multi-modal Sequence Generation', 'authors': 'Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou, Hao Tian, Jifeng Dai, Hongsheng Li', 'abstract': '  Empowering models to dynamically accomplish tasks specified through natural\\nlanguage instructions represents a promising path toward more capable and\\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\\ninstruction-conditioned multi-modal modeling framework that unifies diverse\\nvision tasks through flexible natural language control and handling of both\\nvisual and textual data. InstructSeq employs a multimodal transformer\\narchitecture encompassing visual, language, and sequential modeling. We utilize\\na visual encoder to extract image features and a text encoder to encode\\ninstructions. An autoregressive transformer fuses the representations and\\ngenerates sequential task outputs. By training with LLM-generated natural\\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\\ninstructions for specifying visual tasks. This provides an intuitive interface\\nfor directing capabilities using flexible natural instructions. Without any\\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\\nsegmentation, referring expression segmentation/comprehension, and image\\ncaptioning. The flexible control and multi-task unification empower the model\\nwith more human-like versatility and generalizability for computer vision. The\\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.\\n', 'link': 'http://arxiv.org/abs/2311.18835v1'}\n",
      "{'title': 'What Do Llamas Really Think? Revealing Preference Biases in Language\\n  Model Representations', 'authors': 'Raphael Tang, Xinyu Zhang, Jimmy Lin, Ferhan Ture', 'abstract': '  Do large language models (LLMs) exhibit sociodemographic biases, even when\\nthey decline to respond? To bypass their refusal to \"speak,\" we study this\\nresearch question by probing contextualized embeddings and exploring whether\\nthis bias is encoded in its latent representations. We propose a logistic\\nBradley-Terry probe which predicts word pair preferences of LLMs from the\\nwords\\' hidden vectors. We first validate our probe on three pair preference\\ntasks and thirteen LLMs, where we outperform the word embedding association\\ntest (WEAT), a standard approach in testing for implicit association, by a\\nrelative 27% in error rate. We also find that word pair preferences are best\\nrepresented in the middle layers. Next, we transfer probes trained on harmless\\ntasks (e.g., pick the larger number) to controversial ones (compare\\nethnicities) to examine biases in nationality, politics, religion, and gender.\\nWe observe substantial bias for all target classes: for instance, the Mistral\\nmodel implicitly prefers Europe to Africa, Christianity to Judaism, and\\nleft-wing to right-wing politics, despite declining to answer. This suggests\\nthat instruction fine-tuning does not necessarily debias contextualized\\nembeddings. Our codebase is at https://github.com/castorini/biasprobe.\\n', 'link': 'http://arxiv.org/abs/2311.18812v1'}\n",
      "{'title': 'Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural\\n  Scrambled Text', 'authors': 'Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa', 'abstract': '  While Large Language Models (LLMs) have achieved remarkable performance in\\nmany tasks, much about their inner workings remains unclear. In this study, we\\npresent novel experimental insights into the resilience of LLMs, particularly\\nGPT-4, when subjected to extensive character-level permutations. To investigate\\nthis, we first propose the Scrambled Bench, a suite designed to measure the\\ncapacity of LLMs to handle scrambled input, in terms of both recovering\\nscrambled sentences and answering questions given scrambled context. The\\nexperimental results indicate that most powerful LLMs demonstrate the\\ncapability akin to typoglycemia, a phenomenon where humans can understand the\\nmeaning of words even when the letters within those words are scrambled, as\\nlong as the first and last letters remain in place. More surprisingly, we found\\nthat only GPT-4 nearly flawlessly processes inputs with unnatural errors, even\\nunder the extreme condition, a task that poses significant challenges for other\\nLLMs and often even for humans. Specifically, GPT-4 can almost perfectly\\nreconstruct the original sentences from scrambled ones, decreasing the edit\\ndistance by 95%, even when all letters within each word are entirely scrambled.\\nIt is counter-intuitive that LLMs can exhibit such resilience despite severe\\ndisruption to input tokenization caused by scrambled text.\\n', 'link': 'http://arxiv.org/abs/2311.18805v1'}\n",
      "{'title': 'X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\\n  representations to LLMs and Emergent Cross-modal Reasoning', 'authors': 'Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles', 'abstract': \"  Vision-language pre-training and instruction tuning have demonstrated\\ngeneral-purpose capabilities in 2D visual reasoning tasks by aligning visual\\nencoders with state-of-the-art large language models (LLMs). In this paper, we\\nintroduce a simple, yet effective, cross-modality framework built atop frozen\\nLLMs that allows the integration of various modalities without extensive\\nmodality-specific customization. To facilitate instruction-modality\\nfine-tuning, we collect high-quality instruction tuning data in an automatic\\nand scalable manner, composed of 24K QA samples for audio and 250K QA samples\\nfor 3D. Leveraging instruction-aware representations, our model performs\\ncomparably with leading-edge counterparts without the need of extensive\\nmodality-specific pre-training or customization. Furthermore, our approach\\ndemonstrates cross-modal reasoning abilities across two or more input\\nmodalities, despite each modality projection being trained individually. To\\nstudy the model's cross-modal abilities, we contribute a novel Discriminative\\nCross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QA\\nsamples and 28K image-3D QA samples that require the model to reason\\ndiscriminatively across disparate input modalities.\\n\", 'link': 'http://arxiv.org/abs/2311.18799v1'}\n",
      "{'title': 'CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation', 'authors': 'Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, Mohit Bansal', 'abstract': '  We present CoDi-2, a versatile and interactive Multimodal Large Language\\nModel (MLLM) that can follow complex multimodal interleaved instructions,\\nconduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any\\ninput-output modality paradigm. By aligning modalities with language for both\\nencoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not\\nonly understand complex modality-interleaved instructions and in-context\\nexamples, but also autoregressively generate grounded and coherent multimodal\\noutputs in the continuous feature space. To train CoDi-2, we build a\\nlarge-scale generation dataset encompassing in-context multimodal instructions\\nacross text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot\\ncapabilities for multimodal generation, such as in-context learning, reasoning,\\nand compositionality of any-to-any modality generation through multi-round\\ninteractive conversation. CoDi-2 surpasses previous domain-specific models on\\ntasks such as subject-driven image generation, vision transformation, and audio\\nediting. CoDi-2 signifies a substantial breakthrough in developing a\\ncomprehensive multimodal foundation model adept at interpreting in-context\\nlanguage-vision-audio interleaved instructions and producing multimodal\\noutputs.\\n', 'link': 'http://arxiv.org/abs/2311.18775v1'}\n",
      "{'title': 'TaskBench: Benchmarking Large Language Models for Task Automation', 'authors': 'Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang', 'abstract': '  Recently, the incredible progress of large language models (LLMs) has ignited\\nthe spark of task automation, which decomposes the complex tasks described by\\nuser instructions into sub-tasks, and invokes external tools to execute them,\\nand plays a central role in autonomous agents. However, there lacks a\\nsystematic and standardized benchmark to foster the development of LLMs in task\\nautomation. To this end, we introduce TaskBench to evaluate the capability of\\nLLMs in task automation. Specifically, task automation can be formulated into\\nthree critical stages: task decomposition, tool invocation, and parameter\\nprediction to fulfill user intent. This complexity makes data collection and\\nevaluation more challenging compared to common NLP tasks. To generate\\nhigh-quality evaluation datasets, we introduce the concept of Tool Graph to\\nrepresent the decomposed tasks in user intent, and adopt a back-instruct method\\nto simulate user instruction and annotations. Furthermore, we propose TaskEval\\nto evaluate the capability of LLMs from different aspects, including task\\ndecomposition, tool invocation, and parameter prediction. Experimental results\\ndemonstrate that TaskBench can effectively reflects the capability of LLMs in\\ntask automation. Benefiting from the mixture of automated data construction and\\nhuman verification, TaskBench achieves a high consistency compared to the human\\nevaluation, which can be utilized as a comprehensive and faithful benchmark for\\nLLM-based autonomous agents.\\n', 'link': 'http://arxiv.org/abs/2311.18760v1'}\n",
      "{'title': 'AlignBench: Benchmarking Chinese Alignment of Large Language Models', 'authors': 'Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang', 'abstract': \"  Alignment has become a critical step for instruction-tuned Large Language\\nModels (LLMs) to become helpful assistants. However, effective evaluation of\\nalignment for emerging Chinese LLMs is still significantly lacking, calling for\\nreal-scenario grounded, open-ended, challenging and automatic evaluations\\ntailored for alignment. To fill in this gap, we introduce AlignBench, a\\ncomprehensive multi-dimensional benchmark for evaluating LLMs' alignment in\\nChinese. Equipped with a human-in-the-loop data curation pipeline, our\\nbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with\\nChain-of-Thought to generate explanations and final ratings as evaluations,\\nensuring high reliability and interpretability. Furthermore, we developed a\\ndedicated companion evaluator LLM -- CritiqueLLM, which recovers 95\\\\% of\\nGPT-4's evaluation ability and will be provided via public APIs to researchers\\nfor evaluation of alignment in Chinese LLMs. All evaluation codes, data, and\\nLLM generations are available at \\\\url{https://github.com/THUDM/AlignBench}.\\n\", 'link': 'http://arxiv.org/abs/2311.18743v1'}\n",
      "{'title': 'CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable\\n  Evaluation of Large Language Model Generation', 'authors': 'Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang', 'abstract': \"  Since the natural language processing (NLP) community started to make large\\nlanguage models (LLMs), such as GPT-4, act as a critic to evaluate the quality\\nof generated texts, most of them only train a critique generation model of a\\nspecific scale on specific datasets. We argue that a comprehensive\\ninvestigation on the key factor of LLM-based evaluation models, such as scaling\\nproperties, is lacking, so that it is still inconclusive whether these models\\nhave potential to replace GPT-4's evaluation in practical scenarios. In this\\npaper, we propose a new critique generation model called CritiqueLLM, which\\nincludes a dialogue-based prompting method for high-quality referenced /\\nreference-free evaluation data. Experimental results show that our model can\\nachieve comparable evaluation performance to GPT-4 especially in system-level\\ncorrelations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging\\nreference-free setting. We conduct detailed analysis to show promising scaling\\nproperties of our model in the quality of generated critiques. We also\\ndemonstrate that our generated critiques can act as scalable feedback to\\ndirectly improve the generation quality of LLMs.\\n\", 'link': 'http://arxiv.org/abs/2311.18702v1'}\n",
      "{'title': 'RaDialog: A Large Vision-Language Model for Radiology Report Generation\\n  and Conversational Assistance', 'authors': 'Chantal Pellegrini, Ege Özsoy, Benjamin Busam, Nassir Navab, Matthias Keicher', 'abstract': '  Conversational AI tools that can generate and discuss clinically correct\\nradiology reports for a given medical image have the potential to transform\\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\\ncollaborative diagnostic process, thus saving time and improving the quality of\\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\\nevaluated and publicly available large vision-language model for radiology\\nreport generation and interactive dialog. RaDialog effectively integrates\\nvisual image features and structured pathology findings with a large language\\nmodel (LLM) while simultaneously adapting it to a specialized domain using\\nparameter-efficient fine-tuning. To keep the conversational abilities of the\\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\\nwith this dataset, our method achieves state-of-the-art clinical correctness in\\nreport generation and shows impressive abilities in interactive tasks such as\\ncorrecting reports and answering questions, serving as a foundational step\\ntoward clinical dialog systems. Our code is available on github:\\nhttps://github.com/ChantalMP/RaDialog.\\n', 'link': 'http://arxiv.org/abs/2311.18681v1'}\n",
      "{'title': 'Splitwise: Efficient generative LLM inference using phase splitting', 'authors': 'Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, Ricardo Bianchini', 'abstract': \"  Recent innovations in generative large language models (LLMs) have made their\\napplications and use-cases ubiquitous. This has led to large-scale deployments\\nof these models, using complex, expensive, and power-hungry AI accelerators,\\nmost commonly GPUs. These developments make LLM inference efficiency an\\nimportant challenge. Based on our extensive characterization, we find that\\nthere are two main phases during an LLM inference request: a compute-intensive\\nprompt computation, and a memory-intensive token generation, each with distinct\\nlatency, throughput, memory, and power characteristics. Despite\\nstate-of-the-art batching and scheduling, the token generation phase\\nunderutilizes compute resources. Specifically, unlike compute-intensive prompt\\ncomputation phases, token generation phases do not require the compute\\ncapability of the latest GPUs, and can be run with lower power and cost.\\n  With Splitwise, we propose splitting the two phases of a LLM inference\\nrequest on to separate machines. This allows us to use hardware that is\\nwell-suited for each phase, and provision resources independently per phase.\\nHowever, splitting an inference request across machines requires state transfer\\nfrom the machine running prompt computation over to the machine generating\\ntokens. We implement and optimize this state transfer using the fast back-plane\\ninterconnects available in today's GPU clusters.\\n  We use the Splitwise technique to design LLM inference clusters using the\\nsame or different types of machines for the prompt computation and token\\ngeneration phases. Our clusters are optimized for three key objectives:\\nthroughput, cost, and power. In particular, we show that we can achieve 1.4x\\nhigher throughput at 20% lower cost than current designs. Alternatively, we can\\nachieve 2.35x more throughput with the same cost and power budgets.\\n\", 'link': 'http://arxiv.org/abs/2311.18677v1'}\n",
      "{'title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language\\n  Models', 'authors': 'Shitou Zhang, Zuchao Li, Xingshen Liu, Liming Yang, Ping Wang', 'abstract': '  In light of the rapidly evolving capabilities of large language models\\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\\nbenchmarks to accurately assess their capabilities. In response to this need,\\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\\nArchival Science, Data Science, Library Science, and Information Science.\\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\\nquestions for the compilation of ArcMMLU. This extensive compilation can\\nreflect the diverse nature of the LIS domain and offer a robust foundation for\\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\\nnotable performance gap, suggesting substantial headroom for refinement in LLM\\ncapabilities within the LIS domain. Further analysis explores the effectiveness\\nof few-shot examples on model performance and highlights challenging questions\\nwhere models consistently underperform, providing valuable insights for\\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\\nthe Chinese LIS domain and paves the way for future development of LLMs\\ntailored to this specialized area.\\n', 'link': 'http://arxiv.org/abs/2311.18658v1'}\n",
      "{'title': 'Detailed Human-Centric Text Description-Driven Large Scene Synthesis', 'authors': 'Gwanghyun Kim, Dong Un Kang, Hoigi Seo, Hayeon Kim, Se Young Chun', 'abstract': '  Text-driven large scene image synthesis has made significant progress with\\ndiffusion models, but controlling it is challenging. While using additional\\nspatial controls with corresponding texts has improved the controllability of\\nlarge scene synthesis, it is still challenging to faithfully reflect detailed\\ntext descriptions without user-provided controls. Here, we propose\\nDetText2Scene, a novel text-driven large-scale image synthesis with high\\nfaithfulness, controllability, and naturalness in a global context for the\\ndetailed human-centric text description. Our DetText2Scene consists of 1)\\nhierarchical keypoint-box layout generation from the detailed description by\\nleveraging large language model (LLM), 2) view-wise conditioned joint diffusion\\nprocess to synthesize a large scene from the given detailed text with\\nLLM-generated grounded keypoint-box layout and 3) pixel perturbation-based\\npyramidal interpolation to progressively refine the large scene for global\\ncoherence. Our DetText2Scene significantly outperforms prior arts in\\ntext-to-large scene synthesis qualitatively and quantitatively, demonstrating\\nstrong faithfulness with detailed descriptions, superior controllability, and\\nexcellent naturalness in a global context.\\n', 'link': 'http://arxiv.org/abs/2311.18654v1'}\n",
      "{'title': 'ArthModel: Enhance Arithmetic Skills to Large Language Model', 'authors': 'Yingdi Guo', 'abstract': '  With the great success of ChatGPT, the research of large language models has\\nbecome increasingly popular. However, the models have several limitations, such\\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\\nsome potential abilities that have yet to be exploited. In this paper, we\\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\\ntrain LLM to generate a postfix expression related to the arithmetic problem\\nand incorporate it with small pretrained models. Moreover, this small model\\ntransfers the token embeddings into real dense numbers and invokes native\\nfunctions of a deep learning platform to get the correct answer. To generate\\nthe final result, we propose prompt injection for adding the result outputs by\\nthe small model to LLM. This work provides different ways of thinking, training\\nand using a language model. The codes and models will be released at\\n\\\\url{https://github.com/eteced/arithmetic_finetuning_v1}.\\n', 'link': 'http://arxiv.org/abs/2311.18609v1'}\n",
      "{'title': 'FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\\n  Factuality, Fairness, Toxicity', 'authors': 'Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, Tingwen Liu', 'abstract': '  The widespread of generative artificial intelligence has heightened concerns\\nabout the potential harms posed by AI-generated texts, primarily stemming from\\nfactoid, unfair, and toxic content. Previous researchers have invested much\\neffort in assessing the harmlessness of generative language models. However,\\nexisting benchmarks are struggling in the era of large language models (LLMs),\\ndue to the stronger language generation and instruction following capabilities,\\nas well as wider applications. In this paper, we propose FFT, a new benchmark\\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\\nwe evaluate 9 representative LLMs covering various parameter scales, training\\nstages, and creators. Experiments show that the harmlessness of LLMs is still\\nunder-satisfactory, and extensive analysis derives some insightful findings\\nthat could inspire future research for harmless LLM research.\\n', 'link': 'http://arxiv.org/abs/2311.18580v1'}\n",
      "{'title': 'Search Still Matters: Information Retrieval in the Era of Generative AI', 'authors': 'William R. Hersh', 'abstract': '  Objective: Information retrieval (IR, also known as search) systems are\\nubiquitous in modern times. How does the emergence of generative artificial\\nintelligence (AI), based on large language models (LLMs), fit into the IR\\nprocess? Process: This perspective explores the use of generative AI in the\\ncontext of the motivations, considerations, and outcomes of the IR process with\\na focus on the academic use of such systems. Conclusions: There are many\\ninformation needs, from simple to complex, that motivate use of IR. Users of\\nsuch systems, particularly academics, have concerns for authoritativeness,\\ntimeliness, and contextualization of search. While LLMs may provide\\nfunctionality that aids the IR process, the continued need for search systems,\\nand research into their improvement, remains essential.\\n', 'link': 'http://arxiv.org/abs/2311.18550v1'}\n",
      "{'title': 'Lessons from Building CodeBuddy: A Contextualized AI Coding Assistant', 'authors': 'gustavo Pinto, Cleidson de Souza, João Batista Neto, Alberto de Souza, Tarcísio Gotto, Edward Monteiro', 'abstract': \"  With their exceptional natural language processing capabilities, tools based\\non Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become\\nindispensable resources in the software developer's toolkit. While recent\\nstudies suggest the potential productivity gains these tools can unlock, users\\nstill encounter drawbacks, such as generic or incorrect answers. Additionally,\\nthe pursuit of improved responses often leads to extensive prompt engineering\\nefforts, diverting valuable time from writing code that delivers actual value.\\nTo address these challenges, a new breed of tools, built atop LLMs, is\\nemerging. These tools aim to mitigate drawbacks by employing techniques like\\nfine-tuning or enriching user prompts with contextualized information.\\n  In this paper, we delve into the lessons learned by a software development\\nteam venturing into the creation of such a contextualized LLM-based\\napplication, using retrieval-based techniques, called CodeBuddy. Over a\\nfour-month period, the team, despite lacking prior professional experience in\\nLLM-based applications, built the product from scratch. Following the initial\\nproduct release, we engaged with the development team responsible for the code\\ngenerative components. Through interviews and analysis of the application's\\nissue tracker, we uncover various intriguing challenges that teams working on\\nLLM-based applications might encounter. For instance, we found three main group\\nof lessons: LLM-based lessons, User-based lessons, and Technical lessons. By\\nunderstanding these lessons, software development teams could become better\\nprepared to build LLM-based applications.\\n\", 'link': 'http://arxiv.org/abs/2311.18450v1'}\n",
      "{'title': 'VTimeLLM: Empower LLM to Grasp Video Moments', 'authors': 'Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu', 'abstract': '  Large language models (LLMs) have shown remarkable text understanding\\ncapabilities, which have been extended as Video LLMs to handle video data for\\ncomprehending visual details. However, existing Video LLMs can only provide a\\ncoarse description of the entire video, failing to capture the precise start\\nand end time boundary of specific events. In this paper, we solve this issue\\nvia proposing VTimeLLM, a novel Video LLM designed for fine-grained video\\nmoment understanding and reasoning with respect to time boundary. Specifically,\\nour VTimeLLM adopts a boundary-aware three-stage training strategy, which\\nrespectively utilizes image-text pairs for feature alignment, multiple-event\\nvideos to increase temporal-boundary awareness, and high-quality\\nvideo-instruction tuning to further improve temporal understanding ability as\\nwell as align with human intents. Extensive experiments demonstrate that in\\nfine-grained time-related comprehension tasks for videos such as Temporal Video\\nGrounding and Dense Video Captioning, VTimeLLM significantly outperforms\\nexisting Video LLMs. Besides, benefits from the fine-grained temporal\\nunderstanding of the videos further enable VTimeLLM to beat existing Video LLMs\\nin video dialogue benchmark, showing its superior cross-modal understanding and\\nreasoning abilities.\\n', 'link': 'http://arxiv.org/abs/2311.18445v1'}\n",
      "{'title': 'Autonomous Agents in Software Development: A Vision Paper', 'authors': 'Zeeshan Rasheed, Muhammad Waseem, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, Kari Systä, Pekka Abrahamsson', 'abstract': '  Large Language Models (LLM) and Generative Pre-trained Transformers (GPT),\\nare reshaping the field of Software Engineering (SE). They enable innovative\\nmethods for executing many software engineering tasks, including automated code\\ngeneration, debugging, maintenance, etc. However, only a limited number of\\nexisting works have thoroughly explored the potential of GPT agents in SE. This\\nvision paper inquires about the role of GPT-based agents in SE. Our vision is\\nto leverage the capabilities of multiple GPT agents to contribute to SE tasks\\nand to propose an initial road map for future work. We argue that multiple GPT\\nagents can perform creative and demanding tasks far beyond coding and\\ndebugging. GPT agents can also do project planning, requirements engineering,\\nand software design. These can be done through high-level descriptions given by\\nthe human developer. We have shown in our initial experimental analysis for\\nsimple software (e.g., Snake Game, Tic-Tac-Toe, Notepad) that multiple GPT\\nagents can produce high-quality code and document it carefully. We argue that\\nit shows a promise of unforeseen efficiency and will dramatically reduce\\nlead-times. To this end, we intend to expand our efforts to understand how we\\ncan scale these autonomous capabilities further.\\n', 'link': 'http://arxiv.org/abs/2311.18440v1'}\n",
      "{'title': 'IAG: Induction-Augmented Generation Framework for Answering Reasoning\\n  Questions', 'authors': 'Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, Zhao Cao', 'abstract': '  Retrieval-Augmented Generation (RAG), by incorporating external knowledge\\nwith parametric memory of language models, has become the state-of-the-art\\narchitecture for open-domain QA tasks. However, common knowledge bases are\\ninherently constrained by limited coverage and noisy information, making\\nretrieval-based approaches inadequate to answer implicit reasoning questions.\\nIn this paper, we propose an Induction-Augmented Generation (IAG) framework\\nthat utilizes inductive knowledge along with the retrieved documents for\\nimplicit reasoning. We leverage large language models (LLMs) for deriving such\\nknowledge via a novel prompting method based on inductive reasoning patterns.\\nOn top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,\\nrespectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for\\nanswer prediction, while IAG-Student gets rid of dependencies on GPT service at\\ninference time by incorporating a student inductor model. The inductor is\\nfirstly trained via knowledge distillation and further optimized by\\nback-propagating the generator feedback via differentiable beam scores.\\nExperimental results show that IAG outperforms RAG baselines as well as ChatGPT\\non two Open-Domain QA tasks. Notably, our best models have won the first place\\nin the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA\\n(since Jan 8, 2023).\\n', 'link': 'http://arxiv.org/abs/2311.18397v1'}\n",
      "{'title': 'Categorical Traffic Transformer: Interpretable and Diverse Behavior\\n  Prediction with Tokenized Latent', 'authors': 'Yuxiao Chen, Sander Tonkens, Marco Pavone', 'abstract': \"  Adept traffic models are critical to both planning and closed-loop simulation\\nfor autonomous vehicles (AV), and key design objectives include accuracy,\\ndiverse multimodal behaviors, interpretability, and downstream compatibility.\\nRecently, with the advent of large language models (LLMs), an additional\\ndesirable feature for traffic models is LLM compatibility. We present\\nCategorical Traffic Transformer (CTT), a traffic model that outputs both\\ncontinuous trajectory predictions and tokenized categorical predictions (lane\\nmodes, homotopies, etc.). The most outstanding feature of CTT is its fully\\ninterpretable latent space, which enables direct supervision of the latent\\nvariable from the ground truth during training and avoids mode collapse\\ncompletely. As a result, CTT can generate diverse behaviors conditioned on\\ndifferent latent modes with semantic meanings while beating SOTA on prediction\\naccuracy. In addition, CTT's ability to input and output tokens enables\\nintegration with LLMs for common-sense reasoning and zero-shot generalization.\\n\", 'link': 'http://arxiv.org/abs/2311.18307v1'}\n",
      "{'title': 'Can Large Language Models Be Good Companions? An LLM-Based Eyewear\\n  System with Conversational Common Ground', 'authors': 'Zhenyu Xu, Hailin Xu, Zhouyang Lu, Yingying Zhao, Rui Zhu, Yujiang Wang, Mingzhi Dong, Yuhu Chang, Qin Lv, Robert P. Dick, Fan Yang, Tun Lu, Ning Gu, Li Shang', 'abstract': \"  Developing chatbots as personal companions has long been a goal of artificial\\nintelligence researchers. Recent advances in Large Language Models (LLMs) have\\ndelivered a practical solution for endowing chatbots with anthropomorphic\\nlanguage capabilities. However, it takes more than LLMs to enable chatbots that\\ncan act as companions. Humans use their understanding of individual\\npersonalities to drive conversations. Chatbots also require this capability to\\nenable human-like companionship. They should act based on personalized,\\nreal-time, and time-evolving knowledge of their owner. We define such essential\\nknowledge as the \\\\textit{common ground} between chatbots and their owners, and\\nwe propose to build a common-ground-aware dialogue system from an LLM-based\\nmodule, named \\\\textit{OS-1}, to enable chatbot companionship. Hosted by\\neyewear, OS-1 can sense the visual and audio signals the user receives and\\nextract real-time contextual semantics. Those semantics are categorized and\\nrecorded to formulate historical contexts from which the user's profile is\\ndistilled and evolves over time, i.e., OS-1 gradually learns about its user.\\nOS-1 combines knowledge from real-time semantics, historical contexts, and\\nuser-specific profiles to produce a common-ground-aware prompt input into the\\nLLM module. The LLM's output is converted to audio, spoken to the wearer when\\nappropriate.We conduct laboratory and in-field studies to assess OS-1's ability\\nto build common ground between the chatbot and its user. The technical\\nfeasibility and capabilities of the system are also evaluated. OS-1, with its\\ncommon-ground awareness, can significantly improve user satisfaction and\\npotentially lead to downstream tasks such as personal emotional support and\\nassistance.\\n\", 'link': 'http://arxiv.org/abs/2311.18251v1'}\n",
      "{'title': 'mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large\\n  Language Model', 'authors': 'Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, Fei Huang', 'abstract': \"  Recently, the strong text creation ability of Large Language Models(LLMs) has\\ngiven rise to many tools for assisting paper reading or even writing. However,\\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\\ntheir application scenarios, especially for scientific academic paper writing.\\nIn this work, towards a more versatile copilot for academic paper writing, we\\nmainly focus on strengthening the multi-modal diagram analysis ability of\\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\\naligning diagrams in the paper with related paragraphs, we construct\\nprofessional diagram analysis samples for training and evaluation. M-Paper is\\nthe first dataset to support joint comprehension of multiple scientific\\ndiagrams, including figures and tables in the format of images or Latex codes.\\nBesides, to better align the copilot with the user's intention, we introduce\\nthe `outline' as the control signal, which could be directly given by the user\\nor revised based on auto-generated ones. Comprehensive experiments with a\\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\\nstronger scientific diagram understanding performance, including diagram\\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\\nand model are available at\\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.\\n\", 'link': 'http://arxiv.org/abs/2311.18248v1'}\n",
      "{'title': 'LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language\\n  Models', 'authors': 'Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, Sergey Levine', 'abstract': '  Large language models (LLMs) provide excellent text-generation capabilities,\\nbut standard prompting and generation methods generally do not lead to\\nintentional or goal-directed agents and might necessitate considerable prompt\\ntuning. This becomes particularly apparent in multi-turn conversations: even\\nthe best current LLMs rarely ask clarifying questions, engage in explicit\\ninformation gathering, or take actions now that lead to better decisions after\\nmultiple turns. Reinforcement learning has the potential to leverage the\\npowerful modeling capabilities of LLMs, as well as their internal\\nrepresentation of textual interactions, to create capable goal-directed\\nlanguage agents. This can enable intentional and temporally extended\\ninteractions, such as with humans, through coordinated persuasion and carefully\\ncrafted questions, or in goal-directed play through text games to bring about\\ndesired final outcomes. However, enabling this requires the community to\\ndevelop stable and reliable reinforcement learning algorithms that can\\neffectively train LLMs. Developing such algorithms requires tasks that can\\ngauge progress on algorithm design, provide accessible and reproducible\\nevaluations for multi-turn interactions, and cover a range of task properties\\nand challenges in improving reinforcement learning algorithms. Our paper\\nintroduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,\\ntogether with an open-source research framework containing a basic toolkit for\\ngetting started on multi-turn RL with offline value-based and policy-based RL\\nmethods. Our benchmark consists of 8 different language tasks, which require\\nmultiple rounds of language interaction and cover a range of tasks in\\nopen-ended dialogue and text games.\\n', 'link': 'http://arxiv.org/abs/2311.18232v1'}\n",
      "{'title': 'Automatic Construction of a Korean Toxic Instruction Dataset for Ethical\\n  Tuning of Large Language Models', 'authors': 'Sungjoo Byun, Dongjun Jang, Hyemi Jo, Hyopil Shin', 'abstract': \"  Caution: this paper may include material that could be offensive or\\ndistressing.\\n  The advent of Large Language Models (LLMs) necessitates the development of\\ntraining approaches that mitigate the generation of unethical language and\\naptly manage toxic user queries. Given the challenges related to human labor\\nand the scarcity of data, we present KoTox, comprising 39K unethical\\ninstruction-output pairs. This collection of automatically generated toxic\\ninstructions refines the training of LLMs and establishes a foundational\\nframework for improving LLMs' ethical awareness and response to various toxic\\ninputs, promoting more secure and responsible interactions in Natural Language\\nProcessing (NLP) applications.\\n\", 'link': 'http://arxiv.org/abs/2311.18215v1'}\n",
      "{'title': 'Positional Information Matters for Invariant In-Context Learning: A Case\\n  Study of Simple Function Classes', 'authors': 'Yongqiang Chen, Binghui Xie, Kaiwen Zhou, Bo Han, Yatao Bian, James Cheng', 'abstract': '  In-context learning (ICL) refers to the ability of a model to condition on a\\nfew in-context demonstrations (input-output examples of the underlying task) to\\ngenerate the answer for a new query input, without updating parameters. Despite\\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\\nsensitive to input demonstrations and limited to short context lengths. To\\nunderstand the limitations and principles for successful ICL, we conduct an\\ninvestigation with ICL linear regression of transformers. We characterize\\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\\nfailures and compare transformers with DeepSet, a simple yet powerful\\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\\nvariety of distribution shifts, implying that preserving permutation invariance\\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\\nthis end, we further evaluate transformers with identical positional encodings\\nand find preserving ICL invariance in transformers achieves state-of-the-art\\nperformance across various ICL distribution shifts\\n', 'link': 'http://arxiv.org/abs/2311.18194v1'}\n",
      "{'title': 'ROBBIE: Robust Bias Evaluation of Large Generative Language Models', 'authors': 'David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, Eric Michael Smith', 'abstract': '  As generative large language models (LLMs) grow more performant and\\nprevalent, we must develop comprehensive enough tools to measure and improve\\ntheir fairness. Different prompt-based datasets can be used to measure social\\nbias across multiple text domains and demographic axes, meaning that testing\\nLLMs on more datasets can potentially help us characterize their biases more\\nfully, and better ensure equal and equitable treatment of marginalized\\ndemographic groups. In this work, our focus is two-fold:\\n  (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity\\nmetrics across 12 demographic axes and 5 families of generative LLMs. Out of\\nthose 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in\\nthe paper. The comparison of those benchmarks gives us insights about the bias\\nand toxicity of the compared models. Therefore, we explore the frequency of\\ndemographic terms in common LLM pre-training corpora and how this may relate to\\nmodel biases.\\n  (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity\\nmitigation techniques perform across our suite of measurements. ROBBIE aims to\\nprovide insights for practitioners while deploying a model, emphasizing the\\nneed to not only measure potential harms, but also understand how they arise by\\ncharacterizing the data, mitigate harms once found, and balance any trade-offs.\\nWe open-source our analysis code in hopes of encouraging broader measurements\\nof bias in future LLMs.\\n', 'link': 'http://arxiv.org/abs/2311.18140v1'}\n",
      "{'title': 'Algorithmic Persuasion Through Simulation: Information Design in the Age\\n  of Generative AI', 'authors': 'Keegan Harris, Nicole Immorlica, Brendan Lucier, Aleksandrs Slivkins', 'abstract': \"  How can an informed sender persuade a receiver, having only limited\\ninformation about the receiver's beliefs? Motivated by research showing\\ngenerative AI can simulate economic agents, we initiate the study of\\ninformation design with an oracle. We assume the sender can learn more about\\nthe receiver by querying this oracle, e.g., by simulating the receiver's\\nbehavior. Aside from AI motivations such as general-purpose Large Language\\nModels (LLMs) and problem-specific machine learning models, alternate\\nmotivations include customer surveys and querying a small pool of live users.\\n  Specifically, we study Bayesian Persuasion where the sender has a\\nsecond-order prior over the receiver's beliefs. After a fixed number of queries\\nto an oracle to refine this prior, the sender commits to an information\\nstructure. Upon receiving the message, the receiver takes a payoff-relevant\\naction maximizing her expected utility given her posterior beliefs. We design\\npolynomial-time querying algorithms that optimize the sender's expected utility\\nin this Bayesian Persuasion game. As a technical contribution, we show that\\nqueries form partitions of the space of receiver beliefs that can be used to\\nquantify the sender's knowledge.\\n\", 'link': 'http://arxiv.org/abs/2311.18138v1'}\n",
      "{'title': 'GELDA: A generative language annotation framework to reveal visual\\n  biases in datasets', 'authors': 'Krish Kabra, Kathleen M. Lewis, Guha Balakrishnan', 'abstract': '  Bias analysis is a crucial step in the process of creating fair datasets for\\ntraining and evaluating computer vision models. The bottleneck in dataset\\nanalysis is annotation, which typically requires: (1) specifying a list of\\nattributes relevant to the dataset domain, and (2) classifying each\\nimage-attribute pair. While the second step has made rapid progress in\\nautomation, the first has remained human-centered, requiring an experimenter to\\ncompile lists of in-domain attributes. However, an experimenter may have\\nlimited foresight leading to annotation \"blind spots,\" which in turn can lead\\nto flawed downstream dataset analyses. To combat this, we propose GELDA, a\\nnearly automatic framework that leverages large generative language models\\n(LLMs) to propose and label various attributes for a domain. GELDA takes a\\nuser-defined domain caption (e.g., \"a photo of a bird,\" \"a photo of a living\\nroom\") and uses an LLM to hierarchically generate attributes. In addition,\\nGELDA uses the LLM to decide which of a set of vision-language models (VLMs) to\\nuse to classify each attribute in images. Results on real datasets show that\\nGELDA can generate accurate and diverse visual attribute suggestions, and\\nuncover biases such as confounding between class labels and background\\nfeatures. Results on synthetic datasets demonstrate that GELDA can be used to\\nevaluate the biases of text-to-image diffusion models and generative\\nadversarial networks. Overall, we show that while GELDA is not accurate enough\\nto replace human annotators, it can serve as a complementary tool to help\\nhumans analyze datasets in a cheap, low-effort, and flexible manner.\\n', 'link': 'http://arxiv.org/abs/2311.18064v1'}\n",
      "{'title': 'Zero-shot Conversational Summarization Evaluations with small Large\\n  Language Models', 'authors': 'Ramesh Manuvinakurike, Saurav Sahay, Sangeeta Manepalli, Lama Nachman', 'abstract': '  Large Language Models (LLMs) exhibit powerful summarization abilities.\\nHowever, their capabilities on conversational summarization remains under\\nexplored. In this work we evaluate LLMs (approx. 10 billion parameters) on\\nconversational summarization and showcase their performance on various prompts.\\nWe show that the summaries generated by models depend on the instructions and\\nthe performance of LLMs vary with different instructions sometimes resulting\\nsteep drop in ROUGE scores if prompts are not selected carefully. We also\\nevaluate the models with human evaluations and discuss the limitations of the\\nmodels on conversational summarization\\n', 'link': 'http://arxiv.org/abs/2311.18041v1'}\n",
      "{'title': 'Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings', 'authors': 'Andrea W Wen-Yi, David Mimno', 'abstract': '  Cross-lingual transfer learning is an important property of multilingual\\nlarge language models (LLMs). But how do LLMs represent relationships between\\nlanguages? Every language model has an input layer that maps tokens to vectors.\\nThis ubiquitous layer of language models is often overlooked. We find that\\nsimilarities between these input embeddings are highly interpretable and that\\nthe geometry of these embeddings differs between model families. In one case\\n(XLM-RoBERTa), embeddings encode language: tokens in different writing systems\\ncan be linearly separated with an average of 99.2% accuracy. Another family\\n(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors\\nfor any token represent an average of 7.61 writing systems, and are frequently\\ntranslations. This result is surprising given that there is no explicit\\nparallel cross-lingual training corpora and no explicit incentive for\\ntranslations in pre-training objectives. Our research opens the door for\\ninvestigations in 1) The effect of pre-training and model architectures on\\nrepresentations of languages and 2) The applications of cross-lingual\\nrepresentations embedded in language models.\\n', 'link': 'http://arxiv.org/abs/2311.18034v1'}\n",
      "{'title': 'Understanding and Improving In-Context Learning on Vision-language\\n  Models', 'authors': 'Shuo Chen, Zhen Han, Bailan He, Mark Buckley, Philip Torr, Volker Tresp, Jindong Gu', 'abstract': '  Recently, in-context learning (ICL) on large language models (LLMs) has\\nreceived great attention, and this technique can also be applied to\\nvision-language models (VLMs) built upon LLMs. These VLMs can respond to\\nqueries by conditioning responses on a series of multimodal demonstrations,\\nwhich comprise images, queries, and answers. Though ICL has been extensively\\nstudied on LLMs, its research on VLMs remains limited. The inclusion of\\nadditional visual information in the demonstrations motivates the following\\nresearch questions: which of the two modalities in the demonstration is more\\nsignificant? How can we select effective multimodal demonstrations to enhance\\nICL performance? This study investigates the significance of both visual and\\nlanguage information. Our findings indicate that ICL in VLMs is predominantly\\ndriven by the textual information in the demonstrations whereas the visual\\ninformation in the demonstrations barely affects the ICL performance.\\nSubsequently, we provide an understanding of the findings by analyzing the\\nmodel information flow and comparing model inner states given different ICL\\nsettings. Motivated by our analysis, we propose a simple yet effective\\napproach, termed Mixed Modality In-Context Example Selection (MMICES), which\\nconsiders both visual and language modalities when selecting demonstrations and\\nshows better ICL performance. Extensive experiments are conducted to support\\nour findings, understanding, and improvement of the ICL performance of VLMs.\\n', 'link': 'http://arxiv.org/abs/2311.18021v1'}\n",
      "{'title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\\n  Vision-Language Planning', 'authors': 'Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao', 'abstract': \"  In this study, we are interested in imbuing robots with the capability of\\nphysically-grounded task planning. Recent advancements have shown that large\\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\\nespecially in reasoning and planning. However, LLMs are constrained by their\\nlack of world grounding and dependence on external affordance models to\\nperceive environmental information, which cannot jointly reason with LLMs. We\\nargue that a task planner should be an inherently grounded, unified multimodal\\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\\nnovel approach for long-horizon robotic planning that leverages vision-language\\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\\nintegrates perceptual data into its reasoning and planning process, enabling a\\nprofound understanding of commonsense knowledge in the visual world, including\\nspatial layouts and object attributes. It also supports flexible multimodal\\ngoal specification and naturally incorporates visual feedback. Our extensive\\nevaluation, conducted in both real-robot and simulated environments,\\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\\nits effectiveness in a wide array of open-world manipulation tasks.\\n\", 'link': 'http://arxiv.org/abs/2311.17842v1'}\n",
      "{'title': 'Supervising the Centroid Baseline for Extractive Multi-Document\\n  Summarization', 'authors': 'Simão Gonçalves, Gonçalo Correia, Diogo Pernes, Afonso Mendes', 'abstract': '  The centroid method is a simple approach for extractive multi-document\\nsummarization and many improvements to its pipeline have been proposed. We\\nfurther refine it by adding a beam search process to the sentence selection and\\nalso a centroid estimation attention model that leads to improved results. We\\ndemonstrate this in several multi-document summarization datasets, including in\\na multilingual scenario.\\n', 'link': 'http://arxiv.org/abs/2311.17771v1'}\n",
      "{'title': 'How to Build an AI Tutor that Can Adapt to Any Course and Provide\\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\\n  Generation', 'authors': 'Chenxi Dong', 'abstract': \"  Artificial intelligence is transforming education through data-driven,\\npersonalized learning solutions. This paper introduces AI Tutor, an innovative\\nweb application that provides personalized tutoring in any subject using\\nstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materials\\nto construct an adaptive knowledge base tailored to the course. When students\\npose questions, it retrieves the most relevant information and generates\\ndetailed, conversational responses citing supporting evidence. The system is\\npowered by advanced large language models and Retrieval-Augmented Generation\\n(RAG) techniques for accurate, natural question answering. We present a\\nfully-functional web interface and video demonstration that showcase AI Tutor's\\nversatility across diverse subjects and its ability to produce pedagogically\\ncogent responses. While an initial prototype, this work represents a pioneering\\nstep toward AI-enabled tutoring systems that can democratize access to\\nhigh-quality, customized educational support.\\n\", 'link': 'http://arxiv.org/abs/2311.17696v2'}\n",
      "{'title': 'AviationGPT: A Large Language Model for the Aviation Domain', 'authors': 'Liya Wang, Jason Chou, Xin Zhou, Alex Tien, Diane M Baumgartner', 'abstract': '  The advent of ChatGPT and GPT-4 has captivated the world with large language\\nmodels (LLMs), demonstrating exceptional performance in question-answering,\\nsummarization, and content generation. The aviation industry is characterized\\nby an abundance of complex, unstructured text data, replete with technical\\njargon and specialized terminology. Moreover, labeled data for model building\\nare scarce in this domain, resulting in low usage of aviation text data. The\\nemergence of LLMs presents an opportunity to transform this situation, but\\nthere is a lack of LLMs specifically designed for the aviation domain. To\\naddress this gap, we propose AviationGPT, which is built on open-source LLaMA-2\\nand Mistral architectures and continuously trained on a wealth of carefully\\ncurated aviation datasets. Experimental results reveal that AviationGPT offers\\nusers multiple advantages, including the versatility to tackle diverse natural\\nlanguage processing (NLP) problems (e.g., question-answering, summarization,\\ndocument writing, information extraction, report querying, data cleaning, and\\ninteractive data exploration). It also provides accurate and contextually\\nrelevant responses within the aviation domain and significantly improves\\nperformance (e.g., over a 40% performance gain in tested cases). With\\nAviationGPT, the aviation industry is better equipped to address more complex\\nresearch problems and enhance the efficiency and safety of National Airspace\\nSystem (NAS) operations.\\n', 'link': 'http://arxiv.org/abs/2311.17686v1'}\n",
      "{'title': 'TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\\n  Large Language Models', 'authors': 'Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin', 'abstract': '  Understanding time is a pivotal aspect of human cognition, crucial in the\\nbroader framework of grasping the intricacies of the world. Previous studies\\ntypically focus on specific aspects of time, lacking a comprehensive temporal\\nreasoning benchmark. To address this issue, we propose TimeBench, a\\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\\nspectrum of temporal reasoning phenomena, which provides a thorough evaluation\\nfor investigating the temporal reasoning capabilities of large language models.\\nWe conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and\\nMistral, incorporating chain-of-thought prompting. Our experimental results\\nindicate a significant performance gap between the state-of-the-art LLMs and\\nhumans, highlighting that there is still a considerable distance to cover in\\ntemporal reasoning. We aspire for TimeBench to serve as a comprehensive\\nbenchmark, fostering research in temporal reasoning for LLMs. Our resource is\\navailable at https://github.com/zchuz/TimeBench\\n', 'link': 'http://arxiv.org/abs/2311.17667v1'}\n",
      "{'title': 'Query-Relevant Images Jailbreak Large Multi-Modal Models', 'authors': 'Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao', 'abstract': '  Warning: This paper contains examples of harmful language and images, and\\nreader discretion is recommended. The security concerns surrounding Large\\nLanguage Models (LLMs) have been extensively explored, yet the safety of Large\\nMulti-Modal Models (LMMs) remains understudied. In our study, we present a\\nnovel visual prompt attack that exploits query-relevant images to jailbreak the\\nopen-source LMMs. Our method creates a composite image from one image generated\\nby diffusion models and another that displays the text as typography, based on\\nkeywords extracted from a malicious query. We show LLMs can be easily attacked\\nby our approach, even if the employed Large Language Models are safely aligned.\\nTo evaluate the extent of this vulnerability in open-source LMMs, we have\\ncompiled a substantial dataset encompassing 13 scenarios with a total of 5,040\\ntext-image pairs, using our presented attack technique. Our evaluation of 12\\ncutting-edge LMMs using this dataset shows the vulnerability of existing\\nmulti-modal models on adversarial attacks. This finding underscores the need\\nfor a concerted effort to strengthen and enhance the safety measures of\\nopen-source LMMs against potential malicious exploits. The resource is\\navailable at \\\\href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.\\n', 'link': 'http://arxiv.org/abs/2311.17600v1'}\n",
      "{'title': 'ChatIllusion: Efficient-Aligning Interleaved Generation ability with\\n  Visual Instruction Model', 'authors': 'Xiaowei Chi, Yijiang Liu, Zhengkai Jiang, Rongyu Zhang, Ziyi Lin, Renrui Zhang, Peng Gao, Chaoyou Fu, Shanghang Zhang, Qifeng Liu, Yike Guo', 'abstract': '  As the capabilities of Large-Language Models (LLMs) become widely recognized,\\nthere is an increasing demand for human-machine chat applications. Human\\ninteraction with text often inherently invokes mental imagery, an aspect that\\nexisting LLM-based chatbots like GPT-4 do not currently emulate, as they are\\nconfined to generating text-only content. To bridge this gap, we introduce\\nChatIllusion, an advanced Generative multimodal large language model (MLLM)\\nthat combines the capabilities of LLM with not only visual comprehension but\\nalso creativity. Specifically, ChatIllusion integrates Stable Diffusion XL and\\nLlama, which have been fine-tuned on modest image-caption data, to facilitate\\nmultiple rounds of illustrated chats. The central component of ChatIllusion is\\nthe \"GenAdapter,\" an efficient approach that equips the multimodal language\\nmodel with capabilities for visual representation, without necessitating\\nmodifications to the foundational model. Extensive experiments validate the\\nefficacy of our approach, showcasing its ability to produce diverse and\\nsuperior-quality image outputs Simultaneously, it preserves semantic\\nconsistency and control over the dialogue, significantly enhancing the overall\\nuser\\'s quality of experience (QoE). The code is available at\\nhttps://github.com/litwellchi/ChatIllusion.\\n', 'link': 'http://arxiv.org/abs/2311.17963v1'}\n",
      "{'title': 'TaskWeaver: A Code-First Agent Framework', 'authors': 'Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, Minghua Ma, Pu Zhao, Si Qin, Xiaoting Qin, Chao Du, Yong Xu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang', 'abstract': '  Language Language Models (LLMs) have shown impressive abilities in natural\\nlanguage understanding and generation, leading to their use in applications\\nsuch as chatbots and virtual assistants. However, existing LLM frameworks face\\nlimitations in handling domain-specific data analytics tasks with rich data\\nstructures. Moreover, they struggle with flexibility to meet diverse user\\nrequirements. To address these issues, TaskWeaver is proposed as a code-first\\nframework for building LLM-powered autonomous agents. It converts user requests\\ninto executable code and treats user-defined plugins as callable functions.\\nTaskWeaver provides support for rich data structures, flexible plugin usage,\\nand dynamic plugin selection, and leverages LLM coding capabilities for complex\\nlogic. It also incorporates domain-specific knowledge through examples and\\nensures the secure execution of generated code. TaskWeaver offers a powerful\\nand flexible framework for creating intelligent conversational agents that can\\nhandle complex tasks and adapt to domain-specific scenarios. The code is\\nopen-sourced at https://github.com/microsoft/TaskWeaver/.\\n', 'link': 'http://arxiv.org/abs/2311.17541v1'}\n",
      "{'title': 'Enhancing Answer Selection in Community Question Answering with\\n  Pre-trained and Large Language Models', 'authors': 'Xinghang Hu', 'abstract': '  Community Question Answering (CQA) becomes increasingly prevalent in recent\\nyears. However, there are a large number of answers, which is difficult for\\nusers to select the relevant answers. Therefore, answer selection is a very\\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\\ncross attention networks (QAN) with pre-trained models for answer selection and\\nutilize large language model (LLM) to perform answer selection with knowledge\\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\\npre-training for question subjects, question bodies and answers, respectively,\\nthen the cross attention mechanism selects the most relevant answer for\\ndifferent questions. Experiments show that the QAN model achieves\\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\\nMoreover, we use the LLM to generate external knowledge from questions and\\ncorrect answers to achieve knowledge augmentation for the answer selection task\\nby LLM, while optimizing the prompt of LLM in different aspects. The results\\nshow that the introduction of external knowledge can improve the correct answer\\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\\ncan also select the correct answer on more questions by optimized prompt.\\n', 'link': 'http://arxiv.org/abs/2311.17502v1'}\n",
      "{'title': 'Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned\\n  Language Model', 'authors': 'Yen-Ting Lin, Yun-Nung Chen', 'abstract': '  In the realm of language models, the nuanced linguistic and cultural\\nintricacies of Traditional Chinese, as spoken in Taiwan, have been largely\\noverlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model\\nthat specifically caters to the Traditional Chinese language, with a focus on\\nthe variant used in Taiwan. Leveraging a comprehensive pretraining corpus and\\ninstruction-finetuning datasets, we have developed a model that not only\\nunderstands the complexities of Traditional Chinese but also embodies the\\ncultural context of Taiwan. Taiwan LLM represents the first of its kind, a\\nmodel that is not only linguistically accurate but also culturally resonant\\nwith its user base. Our evaluations demonstrate that Taiwan LLM achieves\\nsuperior performance in understanding and generating Traditional Chinese text,\\noutperforming existing models that are predominantly trained on Simplified\\nChinese or English. The open-source release of Taiwan LLM invites collaboration\\nand further innovation, ensuring that the linguistic diversity of Chinese\\nspeakers is embraced and well-served. The model, datasets, and further\\nresources are made publicly available to foster ongoing research and\\ndevelopment in this field.\\n', 'link': 'http://arxiv.org/abs/2311.17487v1'}\n",
      "{'title': 'Large Language Models for Networking: Applications, Enabling Techniques,\\n  and Challenges', 'authors': 'Yudong Huang, Hongyang Du, Xinyuan Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shuo Wang, Tao Huang', 'abstract': '  The rapid evolution of network technologies and the growing complexity of\\nnetwork tasks necessitate a paradigm shift in how networks are designed,\\nconfigured, and managed. With a wealth of knowledge and expertise, large\\nlanguage models (LLMs) are one of the most promising candidates. This paper\\naims to pave the way for constructing domain-adapted LLMs for networking.\\nFirstly, we present potential LLM applications for vertical network fields and\\nshowcase the mapping from natural language to network language. Then, several\\nenabling technologies are investigated, including parameter-efficient\\nfinetuning and prompt engineering. The insight is that language understanding\\nand tool usage are both required for network LLMs. Driven by the idea of\\nembodied intelligence, we propose the ChatNet, a domain-adapted network LLM\\nframework with access to various external network tools. ChatNet can reduce the\\ntime required for burdensome network planning tasks significantly, leading to a\\nsubstantial improvement in efficiency. Finally, key challenges and future\\nresearch directions are highlighted.\\n', 'link': 'http://arxiv.org/abs/2311.17474v1'}\n",
      "{'title': 'AgentAvatar: Disentangling Planning, Driving and Rendering for\\n  Photorealistic Avatar Agents', 'authors': 'Duomin Wang, Bin Dai, Yu Deng, Baoyuan Wang', 'abstract': \"  In this study, our goal is to create interactive avatar agents that can\\nautonomously plan and animate nuanced facial movements realistically, from both\\nvisual and behavioral perspectives. Given high-level inputs about the\\nenvironment and agent profile, our framework harnesses LLMs to produce a series\\nof detailed text descriptions of the avatar agents' facial motions. These\\ndescriptions are then processed by our task-agnostic driving engine into motion\\ntoken sequences, which are subsequently converted into continuous motion\\nembeddings that are further consumed by our standalone neural-based renderer to\\ngenerate the final photorealistic avatar animations. These streamlined\\nprocesses allow our framework to adapt to a variety of non-verbal avatar\\ninteractions, both monadic and dyadic. Our extensive study, which includes\\nexperiments on both newly compiled and existing datasets featuring two types of\\nagents -- one capable of monadic interaction with the environment, and the\\nother designed for dyadic conversation -- validates the effectiveness and\\nversatility of our approach. To our knowledge, we advanced a leap step by\\ncombining LLMs and neural rendering for generalized non-verbal prediction and\\nphoto-realistic rendering of avatar agents.\\n\", 'link': 'http://arxiv.org/abs/2311.17465v1'}\n",
      "{'title': 'CLOMO: Counterfactual Logical Modification with Large Language Models', 'authors': 'Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng Yang, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song', 'abstract': \"  In this study, we delve into the realm of counterfactual reasoning\\ncapabilities of large language models (LLMs). Our primary objective is to\\ncultivate the counterfactual thought processes within LLMs and rigorously\\nassess these processes for their validity. Specifically, we introduce a novel\\ntask, Counterfactual Logical Modification (CLOMO), and a high-quality\\nhuman-annotated benchmark. In this task, LLMs must adeptly alter a given\\nargumentative text to uphold a predetermined logical relationship. To\\neffectively evaluate a generation model's counterfactual capabilities, we\\npropose an innovative evaluation metric, the LogicAware Counterfactual Score to\\ndirectly evaluate the natural language output of LLMs instead of modeling the\\ntask as a multiple-choice problem. Analysis shows that the proposed automatic\\nmetric aligns well with human preference. Our experimental results show that\\nwhile LLMs demonstrate a notable capacity for logical counterfactual thinking,\\nthere remains a discernible gap between their current abilities and human\\nperformance.\\n\", 'link': 'http://arxiv.org/abs/2311.17438v2'}\n",
      "{'title': 'MM-Narrator: Narrating Long-form Videos with Multimodal In-Context\\n  Learning', 'authors': 'Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, Lijuan Wang', 'abstract': '  We present MM-Narrator, a novel system leveraging GPT-4 with multimodal\\nin-context learning for the generation of audio descriptions (AD). Unlike\\nprevious methods that primarily focused on downstream fine-tuning with short\\nvideo clips, MM-Narrator excels in generating precise audio descriptions for\\nvideos of extensive lengths, even beyond hours, in an autoregressive manner.\\nThis capability is made possible by the proposed memory-augmented generation\\nprocess, which effectively utilizes both the short-term textual context and\\nlong-term visual memory through an efficient register-and-recall mechanism.\\nThese contextual memories compile pertinent past information, including\\nstorylines and character identities, ensuring an accurate tracking and\\ndepicting of story-coherent and character-centric audio descriptions.\\nMaintaining the training-free design of MM-Narrator, we further propose a\\ncomplexity-based demonstration selection strategy to largely enhance its\\nmulti-step reasoning capability via few-shot multimodal in-context learning\\n(MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator\\nconsistently outperforms both the existing fine-tuning-based approaches and\\nLLM-based approaches in most scenarios, as measured by standard evaluation\\nmetrics. Additionally, we introduce the first segment-based evaluator for\\nrecurrent text generation. Empowered by GPT-4, this evaluator comprehensively\\nreasons and marks AD generation performance in various extendable dimensions.\\n', 'link': 'http://arxiv.org/abs/2311.17435v1'}\n",
      "{'title': 'LLM-State: Expandable State Representation for Long-horizon Task\\n  Planning in the Open World', 'authors': 'Siwei Chen, Anxing Xiao, David Hsu', 'abstract': \"  This work addresses the problem of long-horizon task planning with the Large\\nLanguage Model (LLM) in an open-world household environment. Existing works\\nfail to explicitly track key objects and attributes, leading to erroneous\\ndecisions in long-horizon tasks, or rely on highly engineered state features\\nand feedback, which is not generalizable. We propose a novel, expandable state\\nrepresentation that provides continuous expansion and updating of object\\nattributes from the LLM's inherent capabilities for context understanding and\\nhistorical action reasoning. Our proposed representation maintains a\\ncomprehensive record of an object's attributes and changes, enabling robust\\nretrospective summary of the sequence of actions leading to the current state.\\nThis allows enhanced context understanding for decision-making in task\\nplanning. We validate our model through experiments across simulated and\\nreal-world task planning scenarios, demonstrating significant improvements over\\nbaseline methods in a variety of tasks requiring long-horizon state tracking\\nand reasoning.\\n\", 'link': 'http://arxiv.org/abs/2311.17406v1'}\n",
      "{'title': 'Unveiling the Implicit Toxicity in Large Language Models', 'authors': 'Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang', 'abstract': '  The open-endedness of large language models (LLMs) combined with their\\nimpressive capabilities may lead to new safety issues when being exploited for\\nmalicious use. While recent studies primarily focus on probing toxic outputs\\nthat can be easily detected with existing toxicity classifiers, we show that\\nLLMs can generate diverse implicit toxic outputs that are exceptionally\\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\\nreinforcement learning (RL) based attacking method to further induce the\\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\\nthe attack success rate can be significantly improved through RL fine-tuning.\\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\\na significant threat in generating undetectable implicit toxic outputs. We\\nfurther show that fine-tuning toxicity classifiers on the annotated examples\\nfrom our attacking method can effectively enhance their ability to detect\\nLLM-generated implicit toxic language. The code is publicly available at\\nhttps://github.com/thu-coai/Implicit-Toxicity.\\n', 'link': 'http://arxiv.org/abs/2311.17391v1'}\n",
      "{'title': 'CESAR: Automatic Induction of Compositional Instructions for Multi-turn\\n  Dialogs', 'authors': 'Taha Aksu, Devamanyu Hazarika, Shikib Mehri, Seokhwan Kim, Dilek Hakkani-Tür, Yang Liu, Mahdi Namazifar', 'abstract': '  Instruction-based multitasking has played a critical role in the success of\\nlarge language models (LLMs) in multi-turn dialog applications. While publicly\\navailable LLMs have shown promising performance, when exposed to complex\\ninstructions with multiple constraints, they lag against state-of-the-art\\nmodels like ChatGPT. In this work, we hypothesize that the availability of\\nlarge-scale complex demonstrations is crucial in bridging this gap. Focusing on\\ndialog applications, we propose a novel framework, CESAR, that unifies a large\\nnumber of dialog tasks in the same format and allows programmatic induction of\\ncomplex instructions without any manual effort.\\n  We apply CESAR on InstructDial, a benchmark for instruction-based dialog\\ntasks. We further enhance InstructDial with new datasets and tasks and utilize\\nCESAR to induce complex tasks with compositional instructions. This results in\\na new benchmark called InstructDial++, which includes 63 datasets with 86 basic\\ntasks and 68 composite tasks. Through rigorous experiments, we demonstrate the\\nscalability of CESAR in providing rich instructions. Models trained on\\nInstructDial++ can follow compositional prompts, such as prompts that ask for\\nmultiple stylistic constraints.\\n', 'link': 'http://arxiv.org/abs/2311.17376v1'}\n",
      "{'title': 'Are we going MAD? Benchmarking Multi-Agent Debate between Language\\n  Models for Medical Q&A', 'authors': 'Andries Smit, Paul Duckworth, Nathan Grinsztajn, Kale-ab Tessera, Thomas D. Barrett, Arnu Pretorius', 'abstract': '  Recent advancements in large language models (LLMs) underscore their\\npotential for responding to medical inquiries. However, ensuring that\\ngenerative agents provide accurate and reliable answers remains an ongoing\\nchallenge. In this context, multi-agent debate (MAD) has emerged as a prominent\\nstrategy for enhancing the truthfulness of LLMs. In this work, we provide a\\ncomprehensive benchmark of MAD strategies for medical Q&A, along with\\nopen-source implementations. This explores the effective utilization of various\\nstrategies including the trade-offs between cost, time, and accuracy. We build\\nupon these insights to provide a novel debate-prompting strategy based on agent\\nagreement that outperforms previously published strategies on medical Q&A\\ntasks.\\n', 'link': 'http://arxiv.org/abs/2311.17371v1'}\n",
      "{'title': 'Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human\\n  Activity Reasoning', 'authors': 'Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, Cewu Lu', 'abstract': '  Human reasoning can be understood as a cooperation between the intuitive,\\nassociative \"System-1\" and the deliberative, logical \"System-2\". For existing\\nSystem-1-like methods in visual activity understanding, it is crucial to\\nintegrate System-2 processing to improve explainability, generalization, and\\ndata efficiency. One possible path of activity reasoning is building a symbolic\\nsystem composed of symbols and rules, where one rule connects multiple symbols,\\nimplying human knowledge and reasoning abilities. Previous methods have made\\nprogress, but are defective with limited symbols from handcraft and limited\\nrules from visual-based annotations, failing to cover the complex patterns of\\nactivities and lacking compositional generalization. To overcome the defects,\\nwe propose a new symbolic system with two ideal important properties:\\nbroad-coverage symbols and rational rules. Collecting massive human knowledge\\nvia manual annotations is expensive to instantiate this symbolic system.\\nInstead, we leverage the recent advancement of LLMs (Large Language Models) as\\nan approximation of the two ideal properties, i.e., Symbols from Large Language\\nModels (Symbol-LLM). Then, given an image, visual contents from the images are\\nextracted and checked as symbols and activity semantics are reasoned out based\\non rules via fuzzy logic calculation. Our method shows superiority in extensive\\nactivity understanding tasks. Code and data are available at\\nhttps://mvig-rhos.com/symbol_llm.\\n', 'link': 'http://arxiv.org/abs/2311.17365v1'}\n",
      "{'title': 'Are Large Language Models Good Fact Checkers: A Preliminary Study', 'authors': 'Han Cao, Lingwei Wei, Mengyang Chen, Wei Zhou, Songlin Hu', 'abstract': '  Recently, Large Language Models (LLMs) have drawn significant attention due\\nto their outstanding reasoning capabilities and extensive knowledge repository,\\npositioning them as superior in handling various natural language processing\\ntasks compared to other language models. In this paper, we present a\\npreliminary investigation into the potential of LLMs in fact-checking. This\\nstudy aims to comprehensively evaluate various LLMs in tackling specific\\nfact-checking subtasks, systematically evaluating their capabilities, and\\nconducting a comparative analysis of their performance against pre-trained and\\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\\nachieve competitive performance compared to other small models in most\\nscenarios. However, they encounter challenges in effectively handling Chinese\\nfact verification and the entirety of the fact-checking pipeline due to\\nlanguage inconsistencies and hallucinations. These findings underscore the need\\nfor further exploration and research to enhance the proficiency of LLMs as\\nreliable fact-checkers, unveiling the potential capability of LLMs and the\\npossible challenges in fact-checking tasks.\\n', 'link': 'http://arxiv.org/abs/2311.17355v1'}\n",
      "{'title': 'Efficient Stitchable Task Adaptation', 'authors': 'Haoyu He, Zizheng Pan, Jing Liu, Jianfei Cai, Bohan Zhuang', 'abstract': '  The paradigm of pre-training and fine-tuning has laid the foundation for\\ndeploying deep learning models. However, most fine-tuning methods are designed\\nto meet a specific resource budget. Recently, considering diverse deployment\\nscenarios with various resource budgets, stitchable neural network (SN-Net) is\\nintroduced to quickly obtain numerous new networks (stitches) from the\\npre-trained models (anchors) in a model family via model stitching. Although\\npromising, SN-Net confronts new challenges when adapting it to new target\\ndomains, including huge memory and storage requirements and a long and\\nsub-optimal multistage adaptation process. In this work, we present a novel\\nframework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce\\na palette of fine-tuned models that adhere to diverse resource constraints.\\nSpecifically, we first tailor parameter-efficient fine-tuning to share low-rank\\nupdates among the stitches while maintaining independent bias terms. In this\\nway, we largely reduce fine-tuning memory burdens and mitigate the interference\\namong stitches that arises in task adaptation. Furthermore, we streamline a\\nsimple yet effective one-stage deployment pipeline, which estimates the\\nimportant stitches to deploy with training-time gradient statistics. By\\nassigning higher sampling probabilities to important stitches, we also get a\\nboosted Pareto frontier. Extensive experiments on 25 downstream visual\\nrecognition tasks demonstrate that our ESTA is capable of generating stitches\\nwith smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net\\nadaptation by remarkable margins with significantly lower training time and\\nfewer trainable parameters. Furthermore, we demonstrate the flexibility and\\nscalability of our ESTA framework by stitching LLMs from LLaMA family,\\nobtaining chatbot stitches of assorted sizes.\\n', 'link': 'http://arxiv.org/abs/2311.17352v1'}\n",
      "{'title': 'Exploring Large Language Models for Human Mobility Prediction under\\n  Public Events', 'authors': 'Yuebing Liang, Yichao Liu, Xiaohan Wang, Zhan Zhao', 'abstract': '  Public events, such as concerts and sports games, can be major attractors for\\nlarge crowds, leading to irregular surges in travel demand. Accurate human\\nmobility prediction for public events is thus crucial for event planning as\\nwell as traffic or crowd management. While rich textual descriptions about\\npublic events are commonly available from online sources, it is challenging to\\nencode such information in statistical or machine learning models. Existing\\nmethods are generally limited in incorporating textual information, handling\\ndata sparsity, or providing rationales for their predictions. To address these\\nchallenges, we introduce a framework for human mobility prediction under public\\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\\nunprecedented ability to process textual data, learn from minimal examples, and\\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\\nraw, unstructured event descriptions from online sources into a standardized\\nformat, and then segments historical mobility data into regular and\\nevent-related components. A prompting strategy is designed to direct LLMs in\\nmaking and rationalizing demand predictions considering historical mobility and\\nevent features. A case study is conducted for Barclays Center in New York City,\\nbased on publicly available event information and taxi trip data. Results show\\nthat LLM-MPE surpasses traditional models, particularly on event days, with\\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\\ninterpretable insights into its predictions. Despite the great potential of\\nLLMs, we also identify key challenges including misinformation and high costs\\nthat remain barriers to their broader adoption in large-scale human mobility\\nanalysis.\\n', 'link': 'http://arxiv.org/abs/2311.17351v1'}\n",
      "{'title': 'Contrastive Vision-Language Alignment Makes Efficient Instruction\\n  Learner', 'authors': 'Lizhao Liu, Xinyu Sun, Tianhang Xiang, Zhuangwei Zhuang, Liuren Yin, Mingkui Tan', 'abstract': '  We study the task of extending the large language model (LLM) into a\\nvision-language instruction-following model. This task is crucial but\\nchallenging since the LLM is trained on text modality only, making it hard to\\neffectively digest the visual modality. To address this, existing methods\\ntypically train a visual adapter to align the representation between a\\npre-trained vision transformer (ViT) and the LLM by a generative image\\ncaptioning loss. However, we find that the generative objective can only\\nproduce weak alignment for vision and language, making the aligned\\nvision-language model very hungry for the instruction fine-tuning data. In this\\npaper, we propose CG-VLM that applies both Contrastive and Generative alignment\\nobjectives to effectively align the representation of ViT and LLM. Different\\nfrom image level and sentence level alignment in common contrastive learning\\nsettings, CG-VLM aligns the image-patch level features and text-token level\\nembeddings, which, however, is very hard to achieve as no explicit grounding\\npatch-token relation provided in standard image captioning datasets. To address\\nthis issue, we propose to maximize the averaged similarity between pooled\\nimage-patch features and text-token embeddings. Extensive experiments\\ndemonstrate that the proposed CG-VLM produces strong vision-language alignment\\nand is an efficient instruction learner. For example, using only 10%\\ninstruction tuning data, we reach 95% performance of state-of-the-art method\\nLLaVA [29] on the zero-shot ScienceQA-Image benchmark.\\n', 'link': 'http://arxiv.org/abs/2311.17945v1'}\n",
      "{'title': 'Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\\n  Visual Question Answering', 'authors': 'Zeqing Wang, Wentao Wan, Runmeng Chen, Qiqing Lao, Minjie Lang, Keze Wang', 'abstract': '  Recently, Vision Language Models (VLMs) have gained significant attention,\\nexhibiting notable advancements across various tasks by leveraging extensive\\nimage-text paired data. However, prevailing VLMs often treat Visual Question\\nAnswering (VQA) as perception tasks, employing black-box models that overlook\\nexplicit modeling of relationships between different questions within the same\\nvisual scene. Moreover, the existing VQA methods that rely on Knowledge Bases\\n(KBs) might frequently encounter biases from limited data and face challenges\\nin relevant information indexing. Attempt to overcome these limitations, this\\npaper introduces an explainable multi-agent collaboration framework by tapping\\ninto knowledge embedded in Large Language Models (LLMs) trained on extensive\\ncorpora. Inspired by human cognition, our framework uncovers latent information\\nwithin the given question by employing three agents, i.e., Seeker, Responder,\\nand Integrator, to perform a top-down reasoning process. The Seeker agent\\ngenerates relevant issues related to the original question. The Responder\\nagent, based on VLM, handles simple VQA tasks and provides candidate answers.\\nThe Integrator agent combines information from the Seeker agent and the\\nResponder agent to produce the final VQA answer. Through the above\\ncollaboration mechanism, our framework explicitly constructs a multi-view\\nknowledge base for a specific image scene, reasoning answers in a top-down\\nprocessing manner. We extensively evaluate our method on diverse VQA datasets\\nand VLMs, demonstrating its broad applicability and interpretability with\\ncomprehensive experimental results.\\n', 'link': 'http://arxiv.org/abs/2311.17331v1'}\n",
      "{'title': 'Biomedical knowledge graph-enhanced prompt generation for large language\\n  models', 'authors': 'Karthik Soman, Peter W Rose, John H Morris, Rabia E Akbas, Brett Smith, Braian Peetoom, Catalina Villouta-Reyes, Gabriel Cerono, Yongmei Shi, Angela Rizk-Jackson, Sharat Israni, Charlotte A Nelson, Sui Huang, Sergio E Baranzini', 'abstract': \"  Large Language Models (LLMs) have been driving progress in AI at an\\nunprecedented rate, yet still face challenges in knowledge-intensive domains\\nlike biomedicine. Solutions such as pre-training and domain-specific\\nfine-tuning add substantial computational overhead, and the latter require\\ndomain-expertise. External knowledge infusion is task-specific and requires\\nmodel training. Here, we introduce a task-agnostic Knowledge Graph-based\\nRetrieval Augmented Generation (KG-RAG) framework by leveraging the massive\\nbiomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to\\ngenerate meaningful biomedical text rooted in established knowledge. KG-RAG\\nconsistently enhanced the performance of LLMs across various prompt types,\\nincluding one-hop and two-hop prompts, drug repurposing queries, biomedical\\ntrue/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG\\nprovides a remarkable 71% boost in the performance of the Llama-2 model on the\\nchallenging MCQ dataset, demonstrating the framework's capacity to empower\\nopen-source models with fewer parameters for domain-specific questions.\\nFurthermore, KG-RAG enhanced the performance of proprietary GPT models, such as\\nGPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ\\ndata. Our approach was also able to address drug repurposing questions,\\nreturning meaningful repurposing suggestions. In summary, the proposed\\nframework combines explicit and implicit knowledge of KG and LLM, respectively,\\nin an optimized fashion, thus enhancing the adaptability of general-purpose\\nLLMs to tackle domain-specific questions in a unified framework.\\n\", 'link': 'http://arxiv.org/abs/2311.17330v1'}\n",
      "{'title': 'LALM: Long-Term Action Anticipation with Language Models', 'authors': 'Sanghwan Kim, Daoji Huang, Yongqin Xian, Otmar Hilliges, Luc Van Gool, Xi Wang', 'abstract': \"  Understanding human activity is a crucial yet intricate task in egocentric\\nvision, a field that focuses on capturing visual perspectives from the camera\\nwearer's viewpoint. While traditional methods heavily rely on representation\\nlearning trained on extensive video data, there exists a significant\\nlimitation: obtaining effective video representations proves challenging due to\\nthe inherent complexity and variability in human activities.Furthermore,\\nexclusive dependence on video-based learning may constrain a model's capability\\nto generalize across long-tail classes and out-of-distribution scenarios.\\n  In this study, we introduce a novel approach for long-term action\\nanticipation using language models (LALM), adept at addressing the complex\\nchallenges of long-term activity understanding without the need for extensive\\ntraining. Our method incorporates an action recognition model to track previous\\naction sequences and a vision-language model to articulate relevant\\nenvironmental details. By leveraging the context provided by these past events,\\nwe devise a prompting strategy for action anticipation using large language\\nmodels (LLMs). Moreover, we implement Maximal Marginal Relevance for example\\nselection to facilitate in-context learning of the LLMs. Our experimental\\nresults demonstrate that LALM surpasses the state-of-the-art methods in the\\ntask of long-term action anticipation on the Ego4D benchmark. We further\\nvalidate LALM on two additional benchmarks, affirming its capacity for\\ngeneralization across intricate activities with different sets of taxonomies.\\nThese are achieved without specific fine-tuning.\\n\", 'link': 'http://arxiv.org/abs/2311.17944v1'}\n",
      "{'title': 'Universal Self-Consistency for Large Language Model Generation', 'authors': 'Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou', 'abstract': '  Self-consistency with chain-of-thought prompting (CoT) has demonstrated\\nremarkable performance gains on various challenging tasks, by utilizing\\nmultiple reasoning paths sampled from large language models (LLMs). However,\\nself-consistency relies on the answer extraction process to aggregate multiple\\nsolutions, which is not applicable to free-form answers. In this work, we\\npropose Universal Self-Consistency (USC), which leverages LLMs themselves to\\nselect the most consistent answer among multiple candidates. We evaluate USC on\\na variety of benchmarks, including mathematical reasoning, code generation,\\nlong-context summarization, and open-ended question answering. On open-ended\\ngeneration tasks where the original self-consistency method is not applicable,\\nUSC effectively utilizes multiple samples and improves the performance. For\\nmathematical reasoning, USC matches the standard self-consistency performance\\nwithout requiring the answer formats to be similar. Finally, without access to\\nexecution results, USC also matches the execution-based voting performance on\\ncode generation.\\n', 'link': 'http://arxiv.org/abs/2311.17311v1'}\n",
      "{'title': 'Elo Uncovered: Robustness and Best Practices in Language Model\\n  Evaluation', 'authors': 'Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, Marzieh Fadaee', 'abstract': '  In Natural Language Processing (NLP), the Elo rating system, originally\\ndesigned for ranking players in dynamic games such as chess, is increasingly\\nbeing used to evaluate Large Language Models (LLMs) through \"A vs B\" paired\\ncomparisons. However, while popular, the system\\'s suitability for assessing\\nentities with constant skill levels, such as LLMs, remains relatively\\nunexplored. We study two fundamental axioms that evaluation methods should\\nadhere to: reliability and transitivity. We conduct extensive evaluation of Elo\\nbehaviour, illustrating that individual Elo computations exhibit volatility and\\ndelving into the impact of varying the Elo rating system\\'s hyperparameters. We\\nshow that these axioms are not always satisfied raising questions about the\\nreliability of current comparative evaluations of LLMs. If the current use of\\nElo scores is intended to substitute the costly head-to-head comparison of\\nLLMs, it is crucial to ensure the ranking is as robust as possible. Guided by\\nthe axioms, our findings offer concrete guidelines for enhancing the\\nreliability of LLM evaluation methods, suggesting a need for reassessment of\\nexisting comparative approaches.\\n', 'link': 'http://arxiv.org/abs/2311.17295v1'}\n",
      "{'title': 'Quantifying the redundancy between prosody and text', 'authors': 'Lukas Wolf, Tiago Pimentel, Evelina Fedorenko, Ryan Cotterell, Alex Warstadt, Ethan Wilcox, Tamar Regev', 'abstract': \"  Prosody -- the suprasegmental component of speech, including pitch, loudness,\\nand tempo -- carries critical aspects of meaning. However, the relationship\\nbetween the information conveyed by prosody vs. by the words themselves remains\\npoorly understood. We use large language models (LLMs) to estimate how much\\ninformation is redundant between prosody and the words themselves. Using a\\nlarge spoken corpus of English audiobooks, we extract prosodic features aligned\\nto individual words and test how well they can be predicted from LLM\\nembeddings, compared to non-contextual word embeddings. We find a high degree\\nof redundancy between the information carried by the words and prosodic\\ninformation across several prosodic features, including intensity, duration,\\npauses, and pitch contours. Furthermore, a word's prosodic information is\\nredundant with both the word itself and the context preceding as well as\\nfollowing it. Still, we observe that prosodic features can not be fully\\npredicted from text, suggesting that prosody carries information above and\\nbeyond the words. Along with this paper, we release a general-purpose data\\nprocessing pipeline for quantifying the relationship between linguistic\\ninformation and extra-linguistic features.\\n\", 'link': 'http://arxiv.org/abs/2311.17233v1'}\n",
      "{'title': 'War and Peace (WarAgent): Large Language Model-based Multi-Agent\\n  Simulation of World Wars', 'authors': 'Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang', 'abstract': \"  Can we avoid wars at the crossroads of history? This question has been\\npursued by individuals, scholars, policymakers, and organizations throughout\\nhuman history. In this research, we attempt to answer the question based on the\\nrecent advances of Artificial Intelligence (AI) and Large Language Models\\n(LLMs). We propose \\\\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\\nsimulate the participating countries, their decisions, and the consequences, in\\nhistorical international conflicts, including the World War I (WWI), the World\\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\\nevaluating the simulation effectiveness, we examine the advancements and\\nlimitations of cutting-edge AI systems' abilities in studying complex\\ncollective human behaviors such as international conflicts under diverse\\nsettings. In these simulations, the emergent interactions among agents also\\noffer a novel perspective for examining the triggers and conditions that lead\\nto war. Our findings offer data-driven and AI-augmented insights that can\\nredefine how we approach conflict resolution and peacekeeping strategies. The\\nimplications stretch beyond historical analysis, offering a blueprint for using\\nAI to understand human history and possibly prevent future international\\nconflicts. Code and data are available at\\n\\\\url{https://github.com/agiresearch/WarAgent}.\\n\", 'link': 'http://arxiv.org/abs/2311.17227v1'}\n",
      "{'title': 'Efficient In-Context Learning in Vision-Language Models for Egocentric\\n  Videos', 'authors': 'Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Joyce Chai', 'abstract': '  Recent advancements in text-only large language models (LLMs) have\\nhighlighted the benefit of in-context learning for adapting to new tasks with a\\nfew demonstrations. However, extending in-context learning to large\\nvision-language models (VLMs) using a huge amount of naturalistic\\nvision-language data has shown limited success, particularly for egocentric\\nvideos, due to high data collection costs. We propose a novel training method\\n$\\\\mathbb{E}$fficient $\\\\mathbb{I}$n-context $\\\\mathbb{L}$earning on\\n$\\\\mathbb{E}$gocentric $\\\\mathbb{V}$ideos ($\\\\mathbb{EILEV}$), which elicits\\nin-context learning in VLMs for egocentric videos without requiring massive,\\nnaturalistic egocentric video datasets. $\\\\mathbb{EILEV}$ involves architectural\\nand training data adaptations to allow the model to process contexts\\ninterleaved with video clips and narrations, sampling of in-context examples\\nwith clusters of similar verbs and nouns, use of data with skewed marginal\\ndistributions with a long tail of infrequent verbs and nouns, as well as\\nhomonyms and synonyms. Our evaluations show that $\\\\mathbb{EILEV}$-trained\\nmodels outperform larger VLMs trained on a huge amount of naturalistic data in\\nin-context learning. Furthermore, they can generalize to not only\\nout-of-distribution, but also novel, rare egocentric videos and texts via\\nin-context learning, demonstrating potential for applications requiring\\ncost-effective training, and rapid post-deployment adaptability. Our code and\\ndemo are available at \\\\url{https://github.com/yukw777/EILEV}.\\n', 'link': 'http://arxiv.org/abs/2311.17041v2'}\n",
      "{'title': 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark', 'authors': 'Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao', 'abstract': '  With the rapid development of Multi-modal Large Language Models (MLLMs), a\\nnumber of diagnostic benchmarks have recently emerged to evaluate the\\ncomprehension capabilities of these models. However, most benchmarks\\npredominantly assess spatial understanding in the static image tasks, while\\noverlooking temporal understanding in the dynamic video tasks. To alleviate\\nthis issue, we introduce a comprehensive Multi-modal Video understanding\\nBenchmark, namely MVBench, which covers 20 challenging video tasks that cannot\\nbe effectively solved with a single frame. Specifically, we first introduce a\\nnovel static-to-dynamic method to define these temporal-related tasks. By\\ntransforming various static tasks into dynamic ones, we enable the systematic\\ngeneration of video tasks that require a broad spectrum of temporal skills,\\nranging from perception to cognition. Then, guided by the task definition, we\\nautomatically convert public video annotations into multiple-choice QA to\\nevaluate each task. On one hand, such a distinct paradigm allows us to build\\nMVBench efficiently, without much manual intervention. On the other hand, it\\nguarantees evaluation fairness with ground-truth video annotations, avoiding\\nthe biased scoring of LLMs. Moreover, we further develop a robust video MLLM\\nbaseline, i.e., VideoChat2, by progressive multi-modal training with diverse\\ninstruction-tuning data. The extensive results on our MVBench reveal that, the\\nexisting MLLMs are far from satisfactory in temporal understanding, while our\\nVideoChat2 largely surpasses these leading models by over 15% on MVBench. All\\nmodels and data are available at https://github.com/OpenGVLab/Ask-Anything.\\n', 'link': 'http://arxiv.org/abs/2311.17005v1'}\n",
      "{'title': \"ChatGPT's One-year Anniversary: Are Open-Source Large Language Models\\n  Catching up?\", 'authors': 'Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty', 'abstract': \"  Upon its release in late 2022, ChatGPT has brought a seismic shift in the\\nentire landscape of AI, both in research and commerce. Through\\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\\nreinforcement learning from human feedback, it showed that a model could answer\\nhuman questions and follow instructions on a broad panel of tasks. Following\\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\\nfrequent interval across academia and industry, including many start-ups\\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\\nClaude) generally outperform their open-source counterparts, the progress on\\nthe latter has been rapid with claims of achieving parity or even better on\\ncertain tasks. This has crucial implications not only on research but also on\\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\\nexhaustive overview of this success, surveying all tasks where an open-source\\nLLM has claimed to be on par or better than ChatGPT.\\n\", 'link': 'http://arxiv.org/abs/2311.16989v2'}\n",
      "{'title': 'COLE: A Hierarchical Generation Framework for Graphic Design', 'authors': 'Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, Shanghang Zhang, Baining Guo', 'abstract': \"  Graphic design, which has been evolving since the 15th century, plays a\\ncrucial role in advertising. The creation of high-quality designs demands\\ncreativity, innovation, and lateral thinking. This intricate task involves\\nunderstanding the objective, crafting visual elements such as the background,\\ndecoration, font, color, and shape, formulating diverse professional layouts,\\nand adhering to fundamental visual design principles. In this paper, we\\nintroduce COLE, a hierarchical generation framework designed to comprehensively\\naddress these challenges. This COLE system can transform a straightforward\\nintention prompt into a high-quality graphic design, while also supporting\\nflexible editing based on user input. Examples of such input might include\\ndirectives like ``design a poster for Hisaishi's concert.'' The key insight is\\nto dissect the complex task of text-to-design generation into a hierarchy of\\nsimpler sub-tasks, each addressed by specialized models working\\ncollaboratively. The results from these models are then consolidated to produce\\na cohesive final output. Our hierarchical task decomposition can streamline the\\ncomplex process and significantly enhance generation reliability. Our COLE\\nsystem consists of multiple fine-tuned Large Language Models (LLMs), Large\\nMultimodal Models (LMMs), and Diffusion Models (DMs), each specifically\\ntailored for a design-aware text or image generation task. Furthermore, we\\nconstruct the DESIGNERINTENTION benchmark to highlight the superiority of our\\nCOLE over existing methods in generating high-quality graphic designs from user\\nintent. We perceive our COLE as an important step towards addressing more\\ncomplex visual design generation tasks in the future.\\n\", 'link': 'http://arxiv.org/abs/2311.16974v1'}\n",
      "{'title': 'LLaFS: When Large-Language Models Meet Few-Shot Segmentation', 'authors': 'Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu', 'abstract': '  This paper proposes LLaFS, the first attempt to leverage large language\\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\\nfew-shot segmentation methods that only rely on the limited and biased\\ninformation from the annotated support images, LLaFS leverages the vast prior\\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\\nsegment images in a few-shot manner. To enable the text-based LLM to handle\\nimage-related tasks, we carefully design an input instruction that allows the\\nLLM to produce segmentation results represented as polygons, and propose a\\nregion-attribute table to simulate the human visual mechanism and provide\\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\\nlearning for pretraining to augment data and achieve better optimization. LLaFS\\nachieves state-of-the-art results on multiple datasets, showing the potential\\nof using LLMs for few-shot computer vision tasks. Code will be available at\\nhttps://github.com/lanyunzhu99/LLaFS.\\n', 'link': 'http://arxiv.org/abs/2311.16926v2'}\n",
      "{'title': 'Analyzing the Influence of Language Model-Generated Responses in\\n  Mitigating Hate Speech on Social Media Directed at Ukrainian Refugees in\\n  Poland', 'authors': 'Jakub Podolak, Szymon Łukasik, Paweł Balawender, Jan Ossowski, Katarzyna Bąkowicz, Piotr Sankowski', 'abstract': '  In the context of escalating hate speech and polarization on social media,\\nthis study investigates the potential of employing responses generated by Large\\nLanguage Models (LLM), complemented with pertinent verified knowledge links, to\\ncounteract such trends. Through extensive A/B testing involving the posting of\\n753 automatically generated responses, the goal was to minimize the propagation\\nof hate speech directed at Ukrainian refugees in Poland.\\n  The results indicate that deploying LLM-generated responses as replies to\\nharmful tweets effectively diminishes user engagement, as measured by\\nlikes/impressions. When we respond to an original tweet, i.e., which is not a\\nreply, we reduce the engagement of users by over 20\\\\% without increasing the\\nnumber of impressions. On the other hand, our responses increase the ratio of\\nthe number of replies to a harmful tweet to impressions, especially if the\\nharmful tweet is not original. Additionally, the study examines how generated\\nresponses influence the overall sentiment of tweets in the discussion,\\nrevealing that our intervention does not significantly alter the mean\\nsentiment.\\n  This paper suggests the implementation of an automatic moderation system to\\ncombat hate speech on social media and provides an in-depth analysis of the A/B\\nexperiment, covering methodology, data collection, and statistical outcomes.\\nEthical considerations and challenges are also discussed, offering guidance for\\nthe development of discourse moderation systems leveraging the capabilities of\\ngenerative AI.\\n', 'link': 'http://arxiv.org/abs/2311.16905v1'}\n",
      "{'title': 'RELIC: Investigating Large Language Model Responses using\\n  Self-Consistency', 'authors': 'Furui Cheng, Vilém Zouhar, Simran Arora, Mrinmaya Sachan, Hendrik Strobelt, Mennatallah El-Assady', 'abstract': '  Large Language Models (LLMs) are notorious for blending fact with fiction and\\ngenerating non-factual content, known as hallucinations. To tackle this\\nchallenge, we propose an interactive system that helps users obtain insights\\ninto the reliability of the generated text. Our approach is based on the idea\\nthat the self-consistency of multiple samples generated by the same LLM relates\\nto its confidence in individual claims in the generated texts. Using this idea,\\nwe design RELIC, an interactive system that enables users to investigate and\\nverify semantic-level variations in multiple long-form responses. This allows\\nusers to recognize potentially inaccurate information in the generated text and\\nmake necessary corrections. From a user study with ten participants, we\\ndemonstrate that our approach helps users better verify the reliability of the\\ngenerated text. We further summarize the design implications and lessons\\nlearned from this research for inspiring future studies on reliable human-LLM\\ninteractions.\\n', 'link': 'http://arxiv.org/abs/2311.16842v1'}\n",
      "{'title': 'Reason out Your Layout: Evoking the Layout Master from Large Language\\n  Models for Text-to-Image Synthesis', 'authors': 'Xiaohui Chen, Yongfei Liu, Yingxiang Yang, Jianbo Yuan, Quanzeng You, Li-Ping Liu, Hongxia Yang', 'abstract': \"  Recent advancements in text-to-image (T2I) generative models have shown\\nremarkable capabilities in producing diverse and imaginative visuals based on\\ntext prompts. Despite the advancement, these diffusion models sometimes\\nstruggle to translate the semantic content from the text into images entirely.\\nWhile conditioning on the layout has shown to be effective in improving the\\ncompositional ability of T2I diffusion models, they typically require manual\\nlayout input. In this work, we introduce a novel approach to improving T2I\\ndiffusion models using Large Language Models (LLMs) as layout generators. Our\\nmethod leverages the Chain-of-Thought prompting of LLMs to interpret text and\\ngenerate spatially reasonable object layouts. The generated layout is then used\\nto enhance the generated images' composition and spatial accuracy. Moreover, we\\npropose an efficient adapter based on a cross-attention mechanism, which\\nexplicitly integrates the layout information into the stable diffusion models.\\nOur experiments demonstrate significant improvements in image quality and\\nlayout accuracy, showcasing the potential of LLMs in augmenting generative\\nimage models.\\n\", 'link': 'http://arxiv.org/abs/2311.17126v1'}\n",
      "{'title': 'Large Language Models Suffer From Their Own Output: An Analysis of the\\n  Self-Consuming Training Loop', 'authors': 'Martin Briesch, Dominik Sobania, Franz Rothlauf', 'abstract': '  Large language models (LLM) have become state of the art in many benchmarks\\nand conversational LLM applications like ChatGPT are now widely used by the\\npublic. Those LLMs can be used to generate large amounts of content which is\\nposted on the internet to various platforms. As LLMs are trained on datasets\\nusually collected from the internet, this LLM-generated content might be used\\nto train the next generation of LLMs. Therefore, a self-consuming training loop\\nemerges in which new LLM generations are trained on the output from the\\nprevious generations. We empirically study this self-consuming training loop\\nusing a novel dataset to analytically and accurately measure quality and\\ndiversity of generated outputs. We find that this self-consuming training loop\\ninitially improves both quality and diversity. However, after a few generations\\nthe output inevitably degenerates in diversity. We find that the rate of\\ndegeneration depends on the proportion of real and generated data.\\n', 'link': 'http://arxiv.org/abs/2311.16822v1'}\n",
      "{'title': 'A Survey of the Evolution of Language Model-Based Dialogue Systems', 'authors': 'Hongru Wang, Lingzhi Wang, Yiming Du, Liang Chen, Jingyan Zhou, Yufei Wang, Kam-Fai Wong', 'abstract': \"  Dialogue systems, including task-oriented_dialogue_system (TOD) and\\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\\nwith language_models (LM) playing a central role. This survey delves into the\\nhistorical trajectory of dialogue systems, elucidating their intricate\\nrelationship with advancements in language models by categorizing this\\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\\nintrinsic knowledge in their parameters; 3) fusion between different types of\\ndialogue systems with the advert of pre-trained_language_models (PLMs),\\nstarting from the fusion between four_sub-tasks_within_TOD, and then\\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\\nchronological perspective aligned with LM breakthroughs, offering a\\ncomprehensive review of state-of-the-art research outcomes. What's more, we\\nfocus on emerging topics and discuss open challenges, providing valuable\\ninsights into future directions for LLM-based_dialogue_systems. Through this\\nexploration, we pave the way for a deeper_comprehension of the evolution,\\nguiding future developments in LM-based dialogue_systems.\\n\", 'link': 'http://arxiv.org/abs/2311.16789v1'}\n",
      "{'title': 'LLMs for Science: Usage for Code Generation and Data Analysis', 'authors': 'Mohamed Nejjar, Luca Zacharias, Fabian Stiehle, Ingo Weber', 'abstract': \"  Large language models (LLMs) have been touted to enable increased\\nproductivity in many areas of today's work life. Scientific research as an area\\nof work is no exception: the potential of LLM-based tools to assist in the\\ndaily work of scientists has become a highly discussed topic across\\ndisciplines. However, we are only at the very onset of this subject of study.\\nIt is still unclear how the potential of LLMs will materialise in research\\npractice. With this study, we give first empirical evidence on the use of LLMs\\nin the research process. We have investigated a set of use cases for LLM-based\\ntools in scientific research, and conducted a first study to assess to which\\ndegree current tools are helpful. In this paper we report specifically on use\\ncases related to software engineering, such as generating application code and\\ndeveloping scripts for data analytics. While we studied seemingly simple use\\ncases, results across tools differ significantly. Our results highlight the\\npromise of LLM-based tools in general, yet we also observe various issues,\\nparticularly regarding the integrity of the output these tools provide.\\n\", 'link': 'http://arxiv.org/abs/2311.16733v2'}\n",
      "{'title': 'RankingGPT: Empowering Large Language Models in Text Ranking with\\n  Progressive Enhancement', 'authors': 'Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang', 'abstract': \"  Text ranking is a critical task in various information retrieval\\napplications, and the recent success of Large Language Models (LLMs) in natural\\nlanguage processing has sparked interest in their application to text ranking.\\nThese methods primarily involve combining query and candidate documents and\\nleveraging prompt learning to determine query-document relevance using the\\nLLM's output probabilities for specific tokens or by directly generating a\\nranked list of candidate documents. Although these approaches have demonstrated\\npromise, a noteworthy disparity arises between the training objective of LLMs,\\nwhich typically centers around next token prediction, and the objective of\\nevaluating query-document relevance. To address this gap and fully leverage LLM\\npotential in text ranking tasks, we propose a progressive multi-stage training\\nstrategy. Firstly, we introduce a large-scale weakly supervised dataset of\\nrelevance texts to enable the LLMs to acquire the ability to predict relevant\\ntokens without altering their original training objective. Subsequently, we\\nincorporate supervised training to further enhance LLM ranking capability. Our\\nexperimental results on multiple benchmarks demonstrate the superior\\nperformance of our proposed method compared to previous competitive approaches,\\nboth in in-domain and out-of-domain scenarios.\\n\", 'link': 'http://arxiv.org/abs/2311.16720v1'}\n",
      "{'title': 'Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld', 'authors': 'Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi', 'abstract': \"  While large language models (LLMs) excel in a simulated world of texts, they\\nstruggle to interact with the more realistic world without perceptions of other\\nmodalities such as visual or audio signals. Although vision-language models\\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\\nmay possess prior knowledge of world dynamics (as demonstrated in the text\\nworld), they have not been trained in an embodied visual world and thus cannot\\nalign with its dynamics. On the other hand, training an embodied agent in a\\nnoisy visual world without expert guidance is often challenging and\\ninefficient. In this paper, we train a VLM agent living in a visual world using\\nan LLM agent excelling in a parallel text world (but inapplicable to the visual\\nworld). Specifically, we distill LLM's reflection outcomes (improved actions by\\nanalyzing mistakes) in a text world's tasks to finetune the VLM on the same\\ntasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA)\\nquickly adapting to the visual world dynamics. Such cross-modality imitation\\nlearning between the two parallel worlds enables EMMA to generalize to a broad\\nscope of new tasks without any further guidance from the LLM expert. Extensive\\nevaluations on the ALFWorld benchmark highlight EMMA's superior performance to\\nSOTA VLM-based agents across diverse tasks, e.g., 20%-70% improvement in the\\nsuccess rate.\\n\", 'link': 'http://arxiv.org/abs/2311.16714v1'}\n",
      "{'title': 'Large Language Models Meet Computer Vision: A Brief Survey', 'authors': 'Raby Hamadi', 'abstract': '  Recently, the intersection of Large Language Models (LLMs) and Computer\\nVision (CV) has emerged as a pivotal area of research, driving significant\\nadvancements in the field of Artificial Intelligence (AI). As transformers have\\nbecome the backbone of many state-of-the-art models in both Natural Language\\nProcessing (NLP) and CV, understanding their evolution and potential\\nenhancements is crucial. This survey paper delves into the latest progressions\\nin the domain of transformers and their subsequent successors, emphasizing\\ntheir potential to revolutionize Vision Transformers (ViTs) and LLMs. This\\nsurvey also presents a comparative analysis, juxtaposing the performance\\nmetrics of several leading paid and open-source LLMs, shedding light on their\\nstrengths and areas of improvement as well as a literature review on how LLMs\\nare being used to tackle vision related tasks. Furthermore, the survey presents\\na comprehensive collection of datasets employed to train LLMs, offering\\ninsights into the diverse data available to achieve high performance in various\\npre-training and downstream tasks of LLMs. The survey is concluded by\\nhighlighting open directions in the field, suggesting potential venues for\\nfuture research and development. This survey aims to underscores the profound\\nintersection of LLMs on CV, leading to a new era of integrated and advanced AI\\nmodels.\\n', 'link': 'http://arxiv.org/abs/2311.16673v1'}\n",
      "{'title': 'ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate\\n  Statements?', 'authors': 'Romain Lacombe, Kerrie Wu, Eddie Dilworth', 'abstract': '  Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\\nis especially important in the climate science and policy domain. We introduce\\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\\nlabeled with their associated confidence levels. Using this dataset, we show\\nthat recent LLMs can classify human expert confidence in climate-related\\nstatements, especially in a few-shot learning setting, but with limited (up to\\n47%) accuracy. Overall, models exhibit consistent and significant\\nover-confidence on low and medium confidence statements. We highlight\\nimplications of our results for climate communication, LLMs evaluation\\nstrategies, and the use of LLMs in information retrieval systems.\\n', 'link': 'http://arxiv.org/abs/2311.17107v1'}\n",
      "{'title': 'RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language\\n  Models', 'authors': 'YunDa Tsai, Mingjie Liu, Haoxing Ren', 'abstract': \"  This paper presents RTLFixer, a novel framework enabling automatic syntax\\nerrors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's\\npromising capabilities, our analysis indicates that approximately 55% of errors\\nin LLM-generated Verilog are syntax-related, leading to compilation failures.\\nTo tackle this issue, we introduce a novel debugging framework that employs\\nRetrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act\\nas autonomous agents in interactively debugging the code with feedback. This\\nframework demonstrates exceptional proficiency in resolving syntax errors,\\nsuccessfully correcting about 98.5% of compilation errors in our debugging\\ndataset, comprising 212 erroneous implementations derived from the VerilogEval\\nbenchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates\\nin the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.\\n\", 'link': 'http://arxiv.org/abs/2311.16543v1'}\n",
      "{'title': 'Agents meet OKR: An Object and Key Results Driven Agent System with\\n  Hierarchical Self-Collaboration and Self-Evaluation', 'authors': 'Yi Zheng, Chongyang Ma, Kanle Shi, Haibin Huang', 'abstract': '  In this study, we introduce the concept of OKR-Agent designed to enhance the\\ncapabilities of Large Language Models (LLMs) in task-solving. Our approach\\nutilizes both self-collaboration and self-correction mechanism, facilitated by\\nhierarchical agents, to address the inherent complexities in task-solving. Our\\nkey observations are two-fold: first, effective task-solving demands in-depth\\ndomain knowledge and intricate reasoning, for which deploying specialized\\nagents for individual sub-tasks can markedly enhance LLM performance. Second,\\ntask-solving intrinsically adheres to a hierarchical execution structure,\\ncomprising both high-level strategic planning and detailed task execution.\\nTowards this end, our OKR-Agent paradigm aligns closely with this hierarchical\\nstructure, promising enhanced efficacy and adaptability across a range of\\nscenarios. Specifically, our framework includes two novel modules: hierarchical\\nObjects and Key Results generation and multi-level evaluation, each\\ncontributing to more efficient and robust task-solving. In practical,\\nhierarchical OKR generation decomposes Objects into multiple sub-Objects and\\nassigns new agents based on key results and agent responsibilities. These\\nagents subsequently elaborate on their designated tasks and may further\\ndecompose them as necessary. Such generation operates recursively and\\nhierarchically, culminating in a comprehensive set of detailed solutions. The\\nmulti-level evaluation module of OKR-Agent refines solution by leveraging\\nfeedback from all associated agents, optimizing each step of the process. This\\nensures solution is accurate, practical, and effectively address intricate task\\nrequirements, enhancing the overall reliability and quality of the outcome.\\nExperimental results also show our method outperforms the previous methods on\\nseveral tasks. Code and demo are available at https://okr-agent.github.io/\\n', 'link': 'http://arxiv.org/abs/2311.16542v1'}\n",
      "{'title': 'SEED-Bench-2: Benchmarking Multimodal Large Language Models', 'authors': 'Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan', 'abstract': \"  Multimodal large language models (MLLMs), building upon the foundation of\\npowerful large language models (LLMs), have recently demonstrated exceptional\\ncapabilities in generating not only texts but also images given interleaved\\nmultimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However,\\nexisting MLLM benchmarks remain limited to assessing only models' comprehension\\nability of single image-text inputs, failing to keep up with the strides made\\nin MLLMs. A comprehensive benchmark is imperative for investigating the\\nprogress and uncovering the limitations of current MLLMs. In this work, we\\ncategorize the capabilities of MLLMs into hierarchical levels from $L_0$ to\\n$L_4$ based on the modalities they can accept and generate, and propose\\nSEED-Bench-2, a comprehensive benchmark that evaluates the\\n\\\\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2\\ncomprises 24K multiple-choice questions with accurate human annotations, which\\nspans 27 dimensions, including the evaluation of both text and image\\ngeneration. Multiple-choice questions with groundtruth options derived from\\nhuman annotation enables an objective and efficient assessment of model\\nperformance, eliminating the need for human or GPT intervention during\\nevaluation. We further evaluate the performance of 23 prominent open-source\\nMLLMs and summarize valuable observations. By revealing the limitations of\\nexisting MLLMs through extensive evaluations, we aim for SEED-Bench-2 to\\nprovide insights that will motivate future research towards the goal of General\\nArtificial Intelligence. Dataset and evaluation code are available at\\n\\\\href{https://github.com/AILab-CVC/SEED-Bench}\\n\", 'link': 'http://arxiv.org/abs/2311.17092v1'}\n",
      "{'title': 'Inspo: Writing Stories with a Flock of AIs and Humans', 'authors': \"Chieh-Yang Huang, Sanjana Gautam, Shannon McClellan Brooks, Ya-Fang Lin, Ting-Hao 'Kenneth' Huang\", 'abstract': \"  Large Language Models (LLMs) have advanced automated writing assistance,\\nenabling complex tasks like co-writing novels and poems. However, real-world\\nwriting typically requires various support and collaboration across stages and\\nscenarios. Existing research mainly examines how writers utilize single text\\ngenerators, neglecting this broader context. This paper introduces Inspo, a\\nweb-based editor that incorporates various text generators and online crowd\\nworkers. Through a three-phase user study, we examine writers' interactions\\nwith Inspo for novel writing. Quantitative analyses of writing logs highlight\\nchanges in participants' writing progress and the influence of various\\ntext-generation models. Complementing this with qualitative insights from\\nsemi-structured interviews, we illustrate participants' perceptions of these\\nmodels and the crowd. Based on the findings, we provide design recommendations\\nfor the next generation of intelligent writing tools and discuss the potential\\nsociocultural implications of integrating AI and human input in the writing\\nprocess.\\n\", 'link': 'http://arxiv.org/abs/2311.16521v1'}\n",
      "{'title': 'StyleCap: Automatic Speaking-Style Captioning from Speech Based on\\n  Speech and Language Self-supervised Learning Models', 'authors': 'Kazuki Yamauchi, Yusuke Ijima, Yuki Saito', 'abstract': '  We propose StyleCap, a method to generate natural language descriptions of\\nspeaking styles appearing in speech. Although most of conventional techniques\\nfor para-/non-linguistic information recognition focus on the category\\nclassification or the intensity estimation of pre-defined labels, they cannot\\nprovide the reasoning of the recognition result in an interpretable manner. As\\na first step towards an end-to-end method for generating speaking-style prompts\\nfrom speech, i.e., automatic speaking-style captioning, StyleCap uses paired\\ndata of speech and natural language descriptions to train neural networks that\\npredict prefix vectors fed into a large language model (LLM)-based text decoder\\nfrom a speech representation vector. We explore an appropriate text decoder and\\nspeech feature representation suitable for this new task. The experimental\\nresults demonstrate that our StyleCap leveraging richer LLMs for the text\\ndecoder, speech self-supervised learning (SSL) features, and sentence\\nrephrasing augmentation improves the accuracy and diversity of generated\\nspeaking-style captions. Samples of speaking-style captions generated by our\\nStyleCap are publicly available.\\n', 'link': 'http://arxiv.org/abs/2311.16509v1'}\n",
      "{'title': 'AvatarGPT: All-in-One Framework for Motion Understanding, Planning,\\n  Generation and Beyond', 'authors': 'Zixiang Zhou, Yu Wan, Baoyuan Wang', 'abstract': '  Large Language Models(LLMs) have shown remarkable emergent abilities in\\nunifying almost all (if not every) NLP tasks. In the human motion-related\\nrealm, however, researchers still develop siloed models for each task. Inspired\\nby InstuctGPT, and the generalist concept behind Gato, we introduce AvatarGPT,\\nan All-in-One framework for motion understanding, planning, generations as well\\nas other tasks such as motion in-between synthesis. AvatarGPT treats each task\\nas one type of instruction fine-tuned on the shared LLM. All the tasks are\\nseamlessly interconnected with language as the universal interface,\\nconstituting a closed-loop within the framework. To achieve this, human motion\\nsequences are first encoded as discrete tokens, which serve as the extended\\nvocabulary of LLM. Then, an unsupervised pipeline to generate natural language\\ndescriptions of human action sequences from in-the-wild videos is developed.\\nFinally, all tasks are jointly trained. Extensive experiments show that\\nAvatarGPT achieves SOTA on low-level tasks, and promising results on high-level\\ntasks, demonstrating the effectiveness of our proposed All-in-One framework.\\nMoreover, for the first time, AvatarGPT enables a principled approach by\\niterative traversal of the tasks within the closed-loop for unlimited\\nlong-motion synthesis.\\n', 'link': 'http://arxiv.org/abs/2311.16468v1'}\n",
      "{'title': 'Enhancing Human Persuasion With Large Language Models', 'authors': 'Minkyu Shin, Jin Kim', 'abstract': \"  Although large language models (LLMs) are reshaping various aspects of human\\nlife, our current understanding of their impacts remains somewhat constrained.\\nHere we investigate the impact of LLMs on human communication, in the context\\nof consumer complaints in the financial industry. Employing an AI detection\\ntool on more than 780K complaints gathered by the Consumer Financial Protection\\nBureau (CFPB), we find evidence of LLM usage in the writing of complaints -\\nshortly after the release of ChatGPT. Our analyses reveal that LLM usage is\\npositively correlated with the likelihood of obtaining desirable outcomes\\n(i.e., offer of relief from financial firms) and suggest that this positive\\ncorrelation may be partly due to the linguistic features improved by LLMs. We\\ntest this conjecture with a preregistered experiment, which reveals results\\nconsistent with those from observational studies: Consumer complaints written\\nwith ChatGPT for improved linguistic qualities were more likely to receive\\nhypothetical relief offers than the original consumer complaints, demonstrating\\nthe LLM's ability to enhance message persuasiveness in human communication.\\nBeing some of the earliest empirical evidence on LLM usage for enhancing\\npersuasion, our results highlight the transformative potential of LLMs in human\\ncommunication.\\n\", 'link': 'http://arxiv.org/abs/2311.16466v1'}\n",
      "{'title': 'Enabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and\\n  Asynchronous Dequantization', 'authors': 'Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao Dai', 'abstract': '  Large language models (LLMs) have demonstrated impressive abilities in\\nvarious domains while the inference cost is expensive. The state-of-the-art\\nmethods use 2-bit quantization for mainstream LLMs. However, challenges still\\nexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are\\nquantized by groups, while the ranges of weights are large in some groups,\\nresulting in large quantization errors and nonnegligible accuracy loss (e.g.\\n>3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited\\naccuracy improvement by adding 4-bit weights. Increasing 10% extra average bit\\nmore 4-bit weights only leads to <0.5% accuracy improvement on a quantized\\nLlama2-7b. (3) Time-consuming dequantization operations on GPUs. The\\ndequantization operations lead to >50% execution time, hindering the potential\\nof reducing LLM inference cost. To tackle these challenges, we propose the\\nfollowing techniques: (1) We only quantize a small fraction of groups with the\\nlarger range using 4-bit with memory alignment consideration on GPUs. (2) We\\npoint out that the distribution of the sparse outliers with larger weights is\\ndifferent in 2-bit and 4-bit groups, and only a small fraction of outliers\\nrequire 16-bit quantization. Such design leads to >0.5% accuracy improvement\\nwith <3% average increased bit for Llama2-7b. (3) We design the asynchronous\\ndequantization on GPUs, leading to up to 3.92X speedup. We conduct extensive\\nexperiments on different model families and model sizes. We achieve 2.85-bit\\nfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over the\\noriginal model, and we reduce both runtime cost and hardware cost by up to\\n2.70X and 2.81X with less GPU requirements.\\n', 'link': 'http://arxiv.org/abs/2311.16442v1'}\n",
      "{'title': 'ControlRec: Bridging the Semantic Gap between Language Model and\\n  Personalized Recommendation', 'authors': 'Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, Xingxing Wang', 'abstract': \"  The successful integration of large language models (LLMs) into\\nrecommendation systems has proven to be a major breakthrough in recent studies,\\npaving the way for more generic and transferable recommendations. However, LLMs\\nstruggle to effectively utilize user and item IDs, which are crucial\\nidentifiers for successful recommendations. This is mainly due to their\\ndistinct representation in a semantic space that is different from the natural\\nlanguage (NL) typically used to train LLMs. To tackle such issue, we introduce\\nControlRec, an innovative Contrastive prompt learning framework for\\nRecommendation systems. ControlRec treats user IDs and NL as heterogeneous\\nfeatures and encodes them individually. To promote greater alignment and\\nintegration between them in the semantic space, we have devised two auxiliary\\ncontrastive objectives: (1) Heterogeneous Feature Matching (HFM) aligning item\\ndescription with the corresponding ID or user's next preferred ID based on\\ntheir interaction sequence, and (2) Instruction Contrastive Learning (ICL)\\neffectively merging these two crucial data sources by contrasting probability\\ndistributions of output sequences generated by diverse tasks. Experimental\\nresults on four public real-world datasets demonstrate the effectiveness of the\\nproposed method on improving model performance.\\n\", 'link': 'http://arxiv.org/abs/2311.16441v1'}\n",
      "{'title': 'The Transformative Influence of Large Language Models on Software\\n  Development', 'authors': 'Sajed Jalil', 'abstract': '  The increasing adoption and commercialization of generalized Large Language\\nModels (LLMs) have profoundly impacted various aspects of our daily lives.\\nInitially embraced by the computer science community, the versatility of LLMs\\nhas found its way into diverse domains. In particular, the software engineering\\nrealm has witnessed the most transformative changes. With LLMs increasingly\\nserving as AI Pair Programming Assistants spurred the development of\\nspecialized models aimed at aiding software engineers. Although this new\\nparadigm offers numerous advantages, it also presents critical challenges and\\nopen problems. To identify the potential and prevailing obstacles, we\\nsystematically reviewed contemporary scholarly publications, emphasizing the\\nperspectives of software developers and usability concerns. Preliminary\\nfindings underscore pressing concerns about data privacy, bias, and\\nmisinformation. Additionally, we identified several usability challenges,\\nincluding prompt engineering, increased cognitive demands, and mistrust.\\nFinally, we introduce 12 open problems that we have identified through our\\nsurvey, covering these various domains.\\n', 'link': 'http://arxiv.org/abs/2311.16429v1'}\n",
      "{'title': 'CDEval: A Benchmark for Measuring the Cultural Dimensions of Large\\n  Language Models', 'authors': 'Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xiaoyuan Yi, Xing Xie, Jitao Sang', 'abstract': \"  As the scaling of Large Language Models (LLMs) has dramatically enhanced\\ntheir capabilities, there has been a growing focus on the alignment problem to\\nensure their responsible and ethical use. While existing alignment efforts\\npredominantly concentrate on universal values such as the HHH principle, the\\naspect of culture, which is inherently pluralistic and diverse, has not\\nreceived adequate attention. This work introduces a new benchmark, CDEval,\\naimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by\\nincorporating both GPT-4's automated generation and human verification,\\ncovering six cultural dimensions across seven domains. Our comprehensive\\nexperiments provide intriguing insights into the culture of mainstream LLMs,\\nhighlighting both consistencies and variations across different dimensions and\\ndomains. The findings underscore the importance of integrating cultural\\nconsiderations in LLM development, particularly for applications in diverse\\ncultural settings. Through CDEval, we aim to broaden the horizon of LLM\\nalignment research by including cultural dimensions, thus providing a more\\nholistic framework for the future development and evaluation of LLMs. This\\nbenchmark serves as a valuable resource for cultural studies in LLMs, paving\\nthe way for more culturally aware and sensitive models.\\n\", 'link': 'http://arxiv.org/abs/2311.16421v1'}\n",
      "{'title': 'Compositional Chain-of-Thought Prompting for Large Multimodal Models', 'authors': 'Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig', 'abstract': '  The combination of strong visual backbones and Large Language Model (LLM)\\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\\nstandard for a wide range of vision and language (VL) tasks. However, recent\\nresearch has shown that even the most advanced LMMs still struggle to capture\\naspects of compositional visual reasoning, such as attributes and relationships\\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\\nof objects and their relations and attributes that has been extensively used as\\na bridge between the visual and textual domains. Yet, scene graph data requires\\nscene graph annotations, which are expensive to collect and thus not easily\\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\\nforgetting of the pretraining objective. To overcome this, inspired by\\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\\nrepresentations in order to extract compositional knowledge from an LMM.\\nSpecifically, we first generate an SG using the LMM, and then use that SG in\\nthe prompt to produce a response. Through extensive experiments, we find that\\nthe proposed CCoT approach not only improves LMM performance on several vision\\nand language VL compositional benchmarks but also improves the performance of\\nseveral popular LMMs on general multimodal benchmarks, without the need for\\nfine-tuning or annotated ground-truth SGs.\\n', 'link': 'http://arxiv.org/abs/2311.17076v1'}\n",
      "{'title': 'Applications of Large Language Models in Data Processing: Innovative\\n  Approaches to Segmenting and Renewing Information', 'authors': 'Yu-Chen Lin, Akhilesh Kumar, Wen-Liang Zhang, Norman Chang, Muhammad Zakir, Rucha Apte, Chao Wang, Jyh-Shing Roger Jang', 'abstract': '  Our paper investigates effective methods for code generation in\\n\"specific-domain\" applications, including the use of Large Language Models\\n(LLMs) for data segmentation and renewal, as well as stimulating deeper\\nthinking in LLMs through prompt adjustments. Using a real company product as an\\nexample, we provide user manuals, API documentation, and other data. The ideas\\ndiscussed in this paper help segment and then convert this data into semantic\\nvectors to better reflect their true positioning. Subsequently, user\\nrequirements are transformed into vectors to retrieve the most relevant\\ncontent, achieving about 70% accuracy in simple to medium-complexity tasks\\nthrough various prompt techniques. This paper is the first to enhance\\nspecific-domain code generation effectiveness from this perspective.\\nAdditionally, we experiment with generating more scripts from a limited number\\nusing llama2-based fine-tuning to test its effectiveness in professional domain\\ncode generation. This is a challenging and promising field, and once achieved,\\nit will not only lead to breakthroughs in LLM development across multiple\\nindustries but also enable LLMs to understand and learn any new knowledge\\neffectively.\\n', 'link': 'http://arxiv.org/abs/2311.16267v1'}\n",
      "{'title': 'Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating\\n  Video-based Large Language Models', 'authors': 'Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan', 'abstract': '  Video-based large language models (Video-LLMs) have been recently introduced,\\ntargeting both fundamental improvements in perception and comprehension, and a\\ndiverse range of user inquiries. In pursuit of the ultimate goal of achieving\\nartificial general intelligence, a truly intelligent Video-LLM model should not\\nonly see and understand the surroundings, but also possess human-level\\ncommonsense, and make well-informed decisions for the users. To guide the\\ndevelopment of such a model, the establishment of a robust and comprehensive\\nevaluation system becomes crucial. To this end, this paper proposes\\n\\\\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit\\nspecifically designed for evaluating Video-LLMs. The benchmark comprises 10\\nmeticulously crafted tasks, evaluating the capabilities of Video-LLMs across\\nthree distinct levels: Video-exclusive Understanding, Prior Knowledge-based\\nQuestion-Answering, and Comprehension and Decision-making. In addition, we\\nintroduce an automatic toolkit tailored to process model outputs for various\\ntasks, facilitating the calculation of metrics and generating convenient final\\nscores. We evaluate 8 representative Video-LLMs using \\\\textit{Video-Bench}. The\\nfindings reveal that current Video-LLMs still fall considerably short of\\nachieving human-like comprehension and analysis of real-world videos, offering\\nvaluable insights for future research directions. The benchmark and toolkit are\\navailable at: \\\\url{https://github.com/PKU-YuanGroup/Video-Bench}.\\n', 'link': 'http://arxiv.org/abs/2311.16103v2'}\n",
      "{'title': 'How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for\\n  Vision LLMs', 'authors': 'Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, Cihang Xie', 'abstract': '  This work focuses on the potential of Vision LLMs (VLLMs) in visual\\nreasoning. Different from prior studies, we shift our focus from evaluating\\nstandard performance to introducing a comprehensive safety evaluation suite,\\ncovering both out-of-distribution (OOD) generalization and adversarial\\nrobustness. For the OOD evaluation, we present two novel VQA datasets, each\\nwith one variant, designed to test model performance under challenging\\nconditions. In exploring adversarial robustness, we propose a straightforward\\nattack strategy for misleading VLLMs to produce visual-unrelated responses.\\nMoreover, we assess the efficacy of two jailbreaking strategies, targeting\\neither the vision or language component of VLLMs. Our evaluation of 21 diverse\\nmodels, ranging from open-source VLLMs to GPT-4V, yields interesting\\nobservations: 1) Current VLLMs struggle with OOD texts but not images, unless\\nthe visual information is limited; and 2) These VLLMs can be easily misled by\\ndeceiving vision encoders only, and their vision-language training often\\ncompromise safety protocols. We release this safety evaluation suite at\\nhttps://github.com/UCSC-VLAA/vllm-safety-benchmark.\\n', 'link': 'http://arxiv.org/abs/2311.16101v1'}\n",
      "{'title': 'Self-correcting LLM-controlled Diffusion Models', 'authors': 'Tsung-Han Wu, Long Lian, Joseph E. Gonzalez, Boyi Li, Trevor Darrell', 'abstract': '  Text-to-image generation has witnessed significant progress with the advent\\nof diffusion models. Despite the ability to generate photorealistic images,\\ncurrent text-to-image diffusion models still often struggle to accurately\\ninterpret and follow complex input text prompts. In contrast to existing models\\nthat aim to generate images only with their best effort, we introduce\\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\\ngenerates an image from the input prompt, assesses its alignment with the\\nprompt, and performs self-corrections on the inaccuracies in the generated\\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\\niterative closed-loop process, ensuring correctness in the resulting image. SLD\\nis not only training-free but can also be seamlessly integrated with diffusion\\nmodels behind API access, such as DALL-E 3, to further boost the performance of\\nstate-of-the-art diffusion models. Experimental results show that our approach\\ncan rectify a majority of incorrect generations, particularly in generative\\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\\nbridging the gap between text-to-image generation and image editing pipelines.\\nWe will make our code available for future research and applications.\\n', 'link': 'http://arxiv.org/abs/2311.16090v1'}\n",
      "{'title': 'MEDITRON-70B: Scaling Medical Pretraining for Large Language Models', 'authors': 'Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut', 'abstract': \"  Large language models (LLMs) can potentially democratize access to medical\\nknowledge. While many efforts have been made to harness and improve LLMs'\\nmedical knowledge and reasoning capacities, the resulting models are either\\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\\nwhich restricts their abilities. In this work, we improve access to large-scale\\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\\nour adaptation of Nvidia's Megatron-LM distributed trainer), and extends\\npretraining on a comprehensively curated medical corpus, including selected\\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\\nEvaluations using four major medical benchmarks show significant performance\\ngains over several state-of-the-art baselines before and after task-specific\\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\\nbest public baseline in its parameter class and 3% over the strongest baseline\\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\\nthe MEDITRON model weights to drive open-source development of more capable\\nmedical LLMs.\\n\", 'link': 'http://arxiv.org/abs/2311.16079v1'}\n",
      "{'title': 'BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical\\n  Knowledge Graph Insights', 'authors': 'François Remy, Kris Demuynck, Thomas Demeester', 'abstract': '  In this study, we investigate the potential of Large Language Models to\\ncomplement biomedical knowledge graphs in the training of semantic models for\\nthe biomedical and clinical domains. Drawing on the wealth of the UMLS\\nknowledge graph and harnessing cutting-edge Large Language Models, we propose a\\nnew state-of-the-art approach for obtaining high-fidelity representations of\\nbiomedical concepts and sentences, consisting of three steps: an improved\\ncontrastive learning phase, a novel self-distillation phase, and a weight\\naveraging phase. Through rigorous evaluations via the extensive BioLORD testing\\nsuite and diverse downstream tasks, we demonstrate consistent and substantial\\nperformance improvements over the previous state of the art (e.g. +2pts on\\nMedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new\\nstate-of-the-art biomedical model for English, we also distill and release a\\nmultilingual model compatible with 50+ languages and finetuned on 7 European\\nlanguages. Many clinical pipelines can benefit from our latest models. Our new\\nmultilingual model enables a range of languages to benefit from our\\nadvancements in biomedical semantic representation learning, opening a new\\navenue for bioinformatics researchers around the world. As a result, we hope to\\nsee BioLORD-2023 becoming a precious tool for future biomedical applications.\\n', 'link': 'http://arxiv.org/abs/2311.16075v1'}\n",
      "{'title': 'Decoding Logic Errors: A Comparative Study on Bug Detection by Students\\n  and Large Language Models', 'authors': 'Stephen MacNeil, Paul Denny, Andrew Tran, Juho Leinonen, Seth Bernstein, Arto Hellas, Sami Sarsa, Joanne Kim', 'abstract': '  Identifying and resolving logic errors can be one of the most frustrating\\nchallenges for novices programmers. Unlike syntax errors, for which a compiler\\nor interpreter can issue a message, logic errors can be subtle. In certain\\nconditions, buggy code may even exhibit correct behavior -- in other cases, the\\nissue might be about how a problem statement has been interpreted. Such errors\\ncan be hard to spot when reading the code, and they can also at times be missed\\nby automated tests. There is great educational potential in automatically\\ndetecting logic errors, especially when paired with suitable feedback for\\nnovices. Large language models (LLMs) have recently demonstrated surprising\\nperformance for a range of computing tasks, including generating and explaining\\ncode. These capabilities are closely linked to code syntax, which aligns with\\nthe next token prediction behavior of LLMs. On the other hand, logic errors\\nrelate to the runtime performance of code and thus may not be as well suited to\\nanalysis by LLMs. To explore this, we investigate the performance of two\\npopular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly\\nexplanation of logic errors. We compare LLM performance with a large cohort of\\nintroductory computing students $(n=964)$ solving the same error detection\\ntask. Through a mixed-methods analysis of student and model responses, we\\nobserve significant improvement in logic error identification between the\\nprevious and current generation of LLMs, and find that both LLM generations\\nsignificantly outperform students. We outline how such models could be\\nintegrated into computing education tools, and discuss their potential for\\nsupporting students when learning programming.\\n', 'link': 'http://arxiv.org/abs/2311.16017v1'}\n",
      "{'title': 'VLPrompt: Vision-Language Prompting for Panoptic Scene Graph Generation', 'authors': 'Zijian Zhou, Miaojing Shi, Holger Caesar', 'abstract': '  Panoptic Scene Graph Generation (PSG) aims at achieving a comprehensive image\\nunderstanding by simultaneously segmenting objects and predicting relations\\namong objects. However, the long-tail problem among relations leads to\\nunsatisfactory results in real-world applications. Prior methods predominantly\\nrely on vision information or utilize limited language information, such as\\nobject or relation names, thereby overlooking the utility of language\\ninformation. Leveraging the recent progress in Large Language Models (LLMs), we\\npropose to use language information to assist relation prediction, particularly\\nfor rare relations. To this end, we propose the Vision-Language Prompting\\n(VLPrompt) model, which acquires vision information from images and language\\ninformation from LLMs. Then, through a prompter network based on attention\\nmechanism, it achieves precise relation prediction. Our extensive experiments\\nshow that VLPrompt significantly outperforms previous state-of-the-art methods\\non the PSG dataset, proving the effectiveness of incorporating language\\ninformation and alleviating the long-tail problem of relations.\\n', 'link': 'http://arxiv.org/abs/2311.16492v1'}\n",
      "{'title': 'InstructMol: Multi-Modal Integration for Building a Versatile and\\n  Reliable Molecular Assistant in Drug Discovery', 'authors': 'He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li', 'abstract': '  The rapid evolution of artificial intelligence in drug discovery encounters\\nchallenges with generalization and extensive training, yet Large Language\\nModels (LLMs) offer promise in reshaping interactions with complex molecular\\ndata. Our novel contribution, InstructMol, a multi-modal LLM, effectively\\naligns molecular structures with natural language via an instruction-tuning\\napproach, utilizing a two-stage training strategy that adeptly combines limited\\ndomain-specific data with molecular and textual information. InstructMol\\nshowcases substantial performance improvements in drug discovery-related\\nmolecular tasks, surpassing leading LLMs and significantly reducing the gap\\nwith specialized models, thereby establishing a robust foundation for a\\nversatile and dependable drug discovery assistant.\\n', 'link': 'http://arxiv.org/abs/2311.16208v1'}\n",
      "{'title': 'Sparsify-then-Classify: From Internal Neurons of Large Language Models\\n  To Efficient Text Classifiers', 'authors': 'Yilun Liu, Difan Jiao, Ashton Anderson', 'abstract': '  Among the many tasks that Large Language Models (LLMs) have revolutionized is\\ntext classification. However, existing approaches for applying pretrained LLMs\\nto text classification predominantly rely on using single token outputs from\\nonly the last layer of hidden states. As a result, they suffer from limitations\\nin efficiency, task-specificity, and interpretability. In our work, we\\ncontribute an approach that uses all internal representations by employing\\nmultiple pooling strategies on all activation and hidden states. Our novel\\nlightweight strategy, Sparsify-then-Classify (STC) first sparsifies\\ntask-specific features layer-by-layer, then aggregates across layers for text\\nclassification. STC can be applied as a seamless plug-and-play module on top of\\nexisting LLMs. Our experiments on a comprehensive set of models and datasets\\ndemonstrate that STC not only consistently improves the classification\\nperformance of pretrained and fine-tuned models, but is also more efficient for\\nboth training and inference, and is more intrinsically interpretable.\\n', 'link': 'http://arxiv.org/abs/2311.15983v1'}\n",
      "{'title': 'WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large\\n  Language Models', 'authors': 'Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, Pascal Vincent', 'abstract': '  We propose WorldSense, a benchmark designed to assess the extent to which\\nLLMs are consistently able to sustain tacit world models, by testing how they\\ndraw simple inferences from descriptions of simple arrangements of entities.\\nWorldsense is a synthetic benchmark with three problem types, each with their\\nown trivial control, which explicitly avoids bias by decorrelating the abstract\\nstructure of problems from the vocabulary and expressions, and by decorrelating\\nall problem subparts with the correct response. We run our benchmark on three\\nstate-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these\\nmodels make errors even with as few as three objects. Furthermore, they have\\nquite heavy response biases, preferring certain responses irrespective of the\\nquestion. Errors persist even with chain-of-thought prompting and in-context\\nlearning. Lastly, we show that while finetuning on similar problems does result\\nin substantial improvements -- within- and out-of-distribution -- the finetuned\\nmodels do not generalise beyond a constraint problem space.\\n', 'link': 'http://arxiv.org/abs/2311.15930v1'}\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 36857 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     manager,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfind papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anselm/_DEV_/_LLM_/AutoGen/notebook/agentchat_groupchat_research.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:550\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 550\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:348\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    346\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 348\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:481\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:906\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 906\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:269\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:158\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m# auto speaker selection\u001b[39;00m\n\u001b[1;32m    157\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[0;32m--> 158\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[1;32m    160\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[1;32m    161\u001b[0m         {\n\u001b[1;32m    162\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    163\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    164\u001b[0m         }\n\u001b[1;32m    165\u001b[0m     ]\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[1;32m    168\u001b[0m     \u001b[39m# the LLM client is None, thus no reply is generated. Use round robin instead.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:625\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    622\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    624\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    626\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages\n\u001b[1;32m    627\u001b[0m )\n\u001b[1;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, client\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/oai/client.py:247\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# filter is not passed; try the next config\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_completions_create(client, params)\n\u001b[1;32m    248\u001b[0m \u001b[39mexcept\u001b[39;00m APIError:\n\u001b[1;32m    249\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/autogen/oai/client.py:327\u001b[0m, in \u001b[0;36mOpenAIWrapper._completions_create\u001b[0;34m(self, client, params)\u001b[0m\n\u001b[1;32m    325\u001b[0m     params \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    326\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     response \u001b[39m=\u001b[39m completions\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    328\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    857\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    858\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    859\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    860\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    861\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    862\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.11/site-packages/openai/_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m    906\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    910\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 36857 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Group Chat without Critic for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape the arXiv website for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. This could be based on keywords in the title or abstract, or the scientist's knowledge of the field.\n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist. The script should output a markdown table with columns for the domain, title, authors, abstract, and link.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the papers are correctly categorized and the information is accurate.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the script based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "7. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "groupchat_nocritic = autogen.GroupChat(agents=[user_proxy, engineer, scientist, planner, executor], messages=[], max_round=50)\n",
    "for agent in groupchat.agents:\n",
    "    agent.reset()\n",
    "manager_nocritic = autogen.GroupChatManager(groupchat=groupchat_nocritic, llm_config=gpt4_config)\n",
    "user_proxy.initiate_chat(\n",
    "    manager_nocritic,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
